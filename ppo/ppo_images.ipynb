{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class SingleStepImageEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Entorno con una sola decisión:\n",
    "      - Observación = 2 imágenes apiladas en un solo tensor\n",
    "      - Acciones: 2 (0 -> escoger imagen A, 1 -> escoger imagen B)\n",
    "      - Recompensas: \n",
    "          * Si action=0, reward=10 con prob 0.75\n",
    "          * Si action=1, reward=10 con prob 0.25\n",
    "      - Episodio termina en un solo paso.\n",
    "    \"\"\"\n",
    "    def __init__(self, imageA, imageB, rewardA=10.0, rewardB=10.0, probA=0.75, probB=0.25):\n",
    "        super().__init__()\n",
    "        # Se asume imageA e imageB son np.array con shape (C, H, W)\n",
    "        assert imageA.shape == imageB.shape, \"Ambas imágenes deben tener la misma forma\"\n",
    "        self.imageA = imageA\n",
    "        self.imageB = imageB\n",
    "        self.rewardA = rewardA\n",
    "        self.rewardB = rewardB\n",
    "        self.probA = probA\n",
    "        self.probB = probB\n",
    "\n",
    "        # Creamos la observación apilada\n",
    "        # Apilamos en el eje de canales: obs_shape = (2*C, H, W)\n",
    "        self.observation = np.concatenate([imageA, imageB], axis=0)\n",
    "        obs_shape = self.observation.shape  # (2*C, H, W)\n",
    "\n",
    "        # Espacios de Gym\n",
    "        self.action_space = spaces.Discrete(2)  # 2 acciones: A=0, B=1\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.done = False\n",
    "        info = {}\n",
    "        return self.observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episodio ya terminado. Llama reset().\")\n",
    "\n",
    "        if action == 0:  # Acción A\n",
    "            reward = self.rewardA if np.random.rand() < self.probA else 0.0\n",
    "        else:            # Acción B\n",
    "            reward = self.rewardB if np.random.rand() < self.probB else 0.0\n",
    "\n",
    "        self.done = True\n",
    "        return self.observation, reward, self.done, False, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN que produce logits (para 2 acciones) y un valor (escalar).\n",
    "    Supongamos la entrada es shape (batch_size, 2*C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=2, num_actions=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # in_channels=2 si son 2 imágenes en gris\n",
    "        # Si fuese RGB, in_channels=6, etc.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "\n",
    "        # Calcula tamaño tras convoluciones: depende de H, W\n",
    "        self.fc1   = nn.Linear(32*6*6, 128)  # <-- Ajustar si la imagen final es 7x7, depende del input\n",
    "        self.actor = nn.Linear(128, num_actions)\n",
    "        self.critic= nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        logits = self.actor(x)\n",
    "        value  = self.critic(x)\n",
    "        return logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def compute_returns_and_advantages(rewards, values, gamma=1.0):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    for r, v in zip(rewards, values):\n",
    "        G = r\n",
    "        A = G - v\n",
    "        returns.append(G)\n",
    "        advantages.append(A)\n",
    "    return returns, advantages\n",
    "\n",
    "def ppo_train(env, policy_net, optimizer, epochs=10, episodes_per_epoch=100, gamma=1.0, epsilon=0.2, entropy_coef=0.01):\n",
    "    policy_net.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        states, actions, rewards, log_probs_old, values_old = [], [], [], [], []\n",
    "\n",
    "        for _ in range(episodes_per_epoch):\n",
    "            obs, _ = env.reset()  # obs shape: (2*C, H, W)\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # (1, 2*C, H, W)\n",
    "\n",
    "            logits, value = policy_net(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            next_obs, reward, done, truncated, info = env.step(action.item())\n",
    "\n",
    "            states.append(obs)  # guardamos la imagen apilada\n",
    "            actions.append(action.item())\n",
    "            rewards.append(reward)\n",
    "            log_probs_old.append(log_prob.item())\n",
    "            values_old.append(value.item())\n",
    "\n",
    "        # Calcular returns y ventajas\n",
    "        returns, advantages = compute_returns_and_advantages(rewards, values_old, gamma=gamma)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        states_tensor       = torch.tensor(states, dtype=torch.float32)\n",
    "        # (batch_size, 2*C, H, W)\n",
    "        actions_tensor      = torch.tensor(actions, dtype=torch.long)\n",
    "        old_log_probs_tensor = torch.tensor(log_probs_old, dtype=torch.float32)\n",
    "        returns_tensor      = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantages_tensor   = torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "        # Forward batch\n",
    "        # States_tensor shape: (batch_size, 2*C, H, W)\n",
    "        logits, values = policy_net(states_tensor)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions_tensor)\n",
    "\n",
    "        ratio = torch.exp(log_probs - old_log_probs_tensor)\n",
    "        ratio_clipped = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "        policy_loss_1 = ratio * advantages_tensor\n",
    "        policy_loss_2 = ratio_clipped * advantages_tensor\n",
    "        policy_loss   = -torch.mean(torch.min(policy_loss_1, policy_loss_2))\n",
    "\n",
    "        value_loss = torch.mean((values.squeeze() - returns_tensor)**2)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss - entropy_coef * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss={loss.item():.3f} PolicyLoss={policy_loss.item():.3f} ValueLoss={value_loss.item():.3f} Entropy={entropy.item():.3f} AvgReward={avg_reward:.2f}\")\n",
    "\n",
    "def training_executor(image_a=None, image_b=None, reward_a=10.0, reward_b=10.0, prob_a=0.75, prob_b=0.25, checkpoint='ppo_checkpoint.pth', model_save_path='ppo_checkpoint.pth'):\n",
    "    imageA = image_a # np.random.rand(1, 28, 28).astype(np.float32)  \n",
    "    imageB = image_b # np.random.rand(1, 28, 28).astype(np.float32)\n",
    "\n",
    "    env = SingleStepImageEnv(imageA, imageB, rewardA=reward_a, rewardB=reward_b, probA=prob_a, probB=prob_b)\n",
    "\n",
    "    # in_channels=2 si es gris; =6 si es RGB (2 imágenes x 3 canales cada una)\n",
    "    policy_net = CNNActorCritic(in_channels=2, num_actions=2)\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    if checkpoint is not None and os.path.exists(checkpoint):\n",
    "        print(f\"Cargando modelo desde {checkpoint}\")\n",
    "        model = torch.load(checkpoint)\n",
    "        policy_net.load_state_dict(model['model_state_dict'])\n",
    "        optimizer.load_state_dict(model['optimizer_state_dict'])\n",
    "        print(f\"Modelo y optimizador cargados correctamente.\")\n",
    "    else:\n",
    "        print(\"Entrenando desde cero.\")\n",
    "\n",
    "    ppo_train(env, policy_net, optimizer,\n",
    "              epochs=10, episodes_per_epoch=100,\n",
    "              gamma=1.0, epsilon=0.2, entropy_coef=0.01)\n",
    "    \n",
    "    # Guardar modelo\n",
    "    torch.save({\n",
    "        \"model_state_dict\": policy_net.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "\n",
    "    print(\"Modelo guardado en\", checkpoint)\n",
    "\n",
    "    # Evaluación\n",
    "    policy_net.eval()\n",
    "    test_rewards = []\n",
    "    for _ in range(1000):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = policy_net(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        _, reward, done, truncated, info = env.step(action)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "    print(f\"\\nRecompensa promedio en 1000 episodios de test: {np.mean(test_rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de imágenes\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # load image\n",
    "    image = cv2.imread(image)\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # resize to 28x28\n",
    "    resized = cv2.resize(gray, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    # normalize\n",
    "    normalized = resized / 255.0\n",
    "\n",
    "    # convert to tensor\n",
    "    tensor = torch.tensor(normalized, dtype=torch.float32).unsqueeze(0)\n",
    "    #tensor = np.asarray(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_a = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/apple.jpg\")\n",
    "image_b = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/cabbage.jpg\")\n",
    "image_c = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/rock.jpg\")\n",
    "image_d = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/stick.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "Epoch 1/10: Loss=21.615 PolicyLoss=3.441 ValueLoss=36.348 Entropy=0.002 AvgReward=5.70\n",
      "Epoch 2/10: Loss=12.411 PolicyLoss=1.287 ValueLoss=22.248 Entropy=0.002 AvgReward=7.10\n",
      "Epoch 3/10: Loss=7.714 PolicyLoss=-0.346 ValueLoss=16.119 Entropy=0.003 AvgReward=8.00\n",
      "Epoch 4/10: Loss=6.891 PolicyLoss=-1.146 ValueLoss=16.074 Entropy=0.004 AvgReward=8.20\n",
      "Epoch 5/10: Loss=7.837 PolicyLoss=-1.290 ValueLoss=18.253 Entropy=0.005 AvgReward=7.90\n",
      "Epoch 6/10: Loss=9.125 PolicyLoss=-1.100 ValueLoss=20.450 Entropy=0.005 AvgReward=7.40\n",
      "Epoch 7/10: Loss=8.752 PolicyLoss=-1.514 ValueLoss=20.531 Entropy=0.005 AvgReward=7.60\n",
      "Epoch 8/10: Loss=10.198 PolicyLoss=-0.925 ValueLoss=22.245 Entropy=0.005 AvgReward=6.90\n",
      "Epoch 9/10: Loss=9.042 PolicyLoss=-1.577 ValueLoss=21.238 Entropy=0.005 AvgReward=7.50\n",
      "Epoch 10/10: Loss=9.024 PolicyLoss=-1.545 ValueLoss=21.138 Entropy=0.005 AvgReward=7.50\n",
      "Modelo guardado en ppo_checkpoint.pth\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 7.50\n"
     ]
    }
   ],
   "source": [
    "training_executor(image_a, image_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "Epoch 1/10: Loss=28.199 PolicyLoss=6.566 ValueLoss=43.265 Entropy=0.001 AvgReward=0.81\n",
      "Epoch 2/10: Loss=26.938 PolicyLoss=6.392 ValueLoss=41.091 Entropy=0.001 AvgReward=0.63\n",
      "Epoch 3/10: Loss=21.515 PolicyLoss=5.622 ValueLoss=31.786 Entropy=0.002 AvgReward=0.77\n",
      "Epoch 4/10: Loss=17.271 PolicyLoss=4.944 ValueLoss=24.655 Entropy=0.003 AvgReward=0.69\n",
      "Epoch 5/10: Loss=12.337 PolicyLoss=4.051 ValueLoss=16.574 Entropy=0.006 AvgReward=0.79\n",
      "Epoch 6/10: Loss=8.735 PolicyLoss=3.279 ValueLoss=10.912 Entropy=0.012 AvgReward=0.80\n",
      "Epoch 7/10: Loss=6.475 PolicyLoss=2.706 ValueLoss=7.538 Entropy=0.022 AvgReward=0.68\n",
      "Epoch 8/10: Loss=4.139 PolicyLoss=2.016 ValueLoss=4.246 Entropy=0.038 AvgReward=0.76\n",
      "Epoch 9/10: Loss=2.759 PolicyLoss=1.515 ValueLoss=2.489 Entropy=0.061 AvgReward=0.74\n",
      "Epoch 10/10: Loss=1.711 PolicyLoss=1.060 ValueLoss=1.305 Entropy=0.092 AvgReward=0.76\n",
      "Modelo guardado en ppo_checkpoint.pth\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 0.73\n"
     ]
    }
   ],
   "source": [
    "training_executor(image_c, image_d, reward_a=1.0, reward_b=1.0, prob_a=0.75, prob_b=0.25, checkpoint='ppo_checkpoint.pth', model_save_path='c_d_ppo_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde c_d_ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "Epoch 1/10: Loss=6.972 PolicyLoss=-0.509 ValueLoss=14.967 Entropy=0.178 AvgReward=1.82\n",
      "Epoch 2/10: Loss=9.476 PolicyLoss=-1.685 ValueLoss=22.325 Entropy=0.222 AvgReward=2.75\n",
      "Epoch 3/10: Loss=7.055 PolicyLoss=-1.120 ValueLoss=16.354 Entropy=0.264 AvgReward=2.00\n",
      "Epoch 4/10: Loss=5.499 PolicyLoss=-0.725 ValueLoss=12.454 Entropy=0.302 AvgReward=1.46\n",
      "Epoch 5/10: Loss=7.840 PolicyLoss=-1.541 ValueLoss=18.769 Entropy=0.336 AvgReward=2.16\n",
      "Epoch 6/10: Loss=9.955 PolicyLoss=-2.251 ValueLoss=24.420 Entropy=0.364 AvgReward=2.78\n",
      "Epoch 7/10: Loss=6.474 PolicyLoss=-1.349 ValueLoss=15.654 Entropy=0.386 AvgReward=1.81\n",
      "Epoch 8/10: Loss=9.790 PolicyLoss=-2.232 ValueLoss=24.052 Entropy=0.404 AvgReward=2.64\n",
      "Epoch 9/10: Loss=9.060 PolicyLoss=-2.131 ValueLoss=22.389 Entropy=0.417 AvgReward=2.50\n",
      "Epoch 10/10: Loss=5.782 PolicyLoss=-1.278 ValueLoss=14.128 Entropy=0.425 AvgReward=1.62\n",
      "Modelo guardado en c_d_ppo_checkpoint.pth\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 2.18\n"
     ]
    }
   ],
   "source": [
    "training_executor(image_b, image_c, reward_a=10.0, reward_b=1.0, prob_a=0.25, prob_b=0.75, checkpoint='c_d_ppo_checkpoint.pth', model_save_path='b_c_ppo_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(image1, image2, reward1, reward2, prob1, prob2, checkpoint):\n",
    "\n",
    "    env = SingleStepImageEnv(image1, image2, rewardA=reward1, rewardB=reward2, probA=prob1, probB=prob2)\n",
    "\n",
    "    # load model\n",
    "    policy_net = CNNActorCritic(in_channels=2, num_actions=2)\n",
    "\n",
    "    if checkpoint is not None and os.path.exists(checkpoint):\n",
    "        print(f\"Cargando modelo desde {checkpoint}\")\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Modelo y optimizador cargados correctamente.\")\n",
    "    else:\n",
    "        print(\"Checkpoint no encontrado, la evaluación no se puede realizar.\")\n",
    "        return -1 \n",
    "    policy_net.eval()\n",
    "    test_rewards = []\n",
    "    for _ in range(1000):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = policy_net(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        _, reward, done, truncated, info = env.step(action)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "    print(f\"\\nRecompensa promedio en 1000 episodios de test: {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde b_c_ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 2.16\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(image_b, image_c, 10.0, 1.0, 0.25, 0.75, 'b_c_ppo_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

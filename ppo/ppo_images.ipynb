{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class SingleStepImageEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Entorno con una sola decisión:\n",
    "      - Observación = 2 imágenes apiladas en un solo tensor\n",
    "      - Acciones: 2 (0 -> escoger imagen A, 1 -> escoger imagen B)\n",
    "      - Recompensas: \n",
    "          * Si action=0, reward=10 con prob 0.75\n",
    "          * Si action=1, reward=10 con prob 0.25\n",
    "      - Episodio termina en un solo paso.\n",
    "    \"\"\"\n",
    "    def __init__(self, imageA, imageB, rewardA=10.0, rewardB=10.0, probA=0.75, probB=0.25):\n",
    "        super().__init__()\n",
    "        # Se asume imageA e imageB son np.array con shape (C, H, W)\n",
    "        assert imageA.shape == imageB.shape, \"Ambas imágenes deben tener la misma forma\"\n",
    "        self.imageA = imageA\n",
    "        self.imageB = imageB\n",
    "        self.rewardA = rewardA\n",
    "        self.rewardB = rewardB\n",
    "        self.probA = probA\n",
    "        self.probB = probB\n",
    "\n",
    "        # Creamos la observación apilada\n",
    "        # Apilamos en el eje de canales: obs_shape = (2*C, H, W)\n",
    "        self.observation = np.concatenate([imageA, imageB], axis=0)\n",
    "        obs_shape = self.observation.shape  # (2*C, H, W)\n",
    "\n",
    "        # Espacios de Gym\n",
    "        self.action_space = spaces.Discrete(2)  # 2 acciones: A=0, B=1\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.done = False\n",
    "        info = {}\n",
    "        return self.observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episodio ya terminado. Llama reset().\")\n",
    "\n",
    "        if action == 0:  # Acción A\n",
    "            reward = self.rewardA if np.random.rand() < self.probA else 0.0\n",
    "        else:            # Acción B\n",
    "            reward = self.rewardB if np.random.rand() < self.probB else 0.0\n",
    "\n",
    "        self.done = True\n",
    "        return self.observation, reward, self.done, False, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN que produce logits (para 2 acciones) y un valor (escalar).\n",
    "    Supongamos la entrada es shape (batch_size, 2*C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=2, num_actions=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # in_channels=2 si son 2 imágenes en gris\n",
    "        # Si fuese RGB, in_channels=6, etc.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "\n",
    "        # Calcula tamaño tras convoluciones: depende de H, W\n",
    "        self.fc1   = nn.Linear(32*6*6, 128)  # <-- Ajustar si la imagen final es 7x7, depende del input\n",
    "        self.actor = nn.Linear(128, num_actions)\n",
    "        self.critic= nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        logits = self.actor(x)\n",
    "        value  = self.critic(x)\n",
    "        return logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def compute_returns_and_advantages(rewards, values, gamma=1.0):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    for r, v in zip(rewards, values):\n",
    "        G = r\n",
    "        A = G - v\n",
    "        returns.append(G)\n",
    "        advantages.append(A)\n",
    "    return returns, advantages\n",
    "\n",
    "def ppo_train(env, policy_net, optimizer, epochs=10, episodes_per_epoch=100, gamma=1.0, epsilon=0.2, entropy_coef=0.01):\n",
    "    policy_net.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        states, actions, rewards, log_probs_old, values_old = [], [], [], [], []\n",
    "\n",
    "        for _ in range(episodes_per_epoch):\n",
    "            obs, _ = env.reset()  # obs shape: (2*C, H, W)\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # (1, 2*C, H, W)\n",
    "\n",
    "            logits, value = policy_net(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            next_obs, reward, done, truncated, info = env.step(action.item())\n",
    "\n",
    "            states.append(obs)  # guardamos la imagen apilada\n",
    "            actions.append(action.item())\n",
    "            rewards.append(reward)\n",
    "            log_probs_old.append(log_prob.item())\n",
    "            values_old.append(value.item())\n",
    "\n",
    "        # Calcular returns y ventajas\n",
    "        returns, advantages = compute_returns_and_advantages(rewards, values_old, gamma=gamma)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        states_tensor       = torch.tensor(states, dtype=torch.float32)\n",
    "        # (batch_size, 2*C, H, W)\n",
    "        actions_tensor      = torch.tensor(actions, dtype=torch.long)\n",
    "        old_log_probs_tensor = torch.tensor(log_probs_old, dtype=torch.float32)\n",
    "        returns_tensor      = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantages_tensor   = torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "        # Forward batch\n",
    "        # States_tensor shape: (batch_size, 2*C, H, W)\n",
    "        logits, values = policy_net(states_tensor)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions_tensor)\n",
    "\n",
    "        ratio = torch.exp(log_probs - old_log_probs_tensor)\n",
    "        ratio_clipped = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "        policy_loss_1 = ratio * advantages_tensor\n",
    "        policy_loss_2 = ratio_clipped * advantages_tensor\n",
    "        policy_loss   = -torch.mean(torch.min(policy_loss_1, policy_loss_2))\n",
    "\n",
    "        value_loss = torch.mean((values.squeeze() - returns_tensor)**2)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss - entropy_coef * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss={loss.item():.3f} PolicyLoss={policy_loss.item():.3f} ValueLoss={value_loss.item():.3f} Entropy={entropy.item():.3f} AvgReward={avg_reward:.2f}\")\n",
    "\n",
    "def training_executor(image_a=None, image_b=None, reward_a=10.0, reward_b=10.0, prob_a=0.75, prob_b=0.25, checkpoint='ppo_checkpoint.pth', model_save_path='ppo_checkpoint.pth'):\n",
    "    imageA = image_a # np.random.rand(1, 28, 28).astype(np.float32)  \n",
    "    imageB = image_b # np.random.rand(1, 28, 28).astype(np.float32)\n",
    "\n",
    "    env = SingleStepImageEnv(imageA, imageB, rewardA=reward_a, rewardB=reward_b, probA=prob_a, probB=prob_b)\n",
    "\n",
    "    # in_channels=2 si es gris; =6 si es RGB (2 imágenes x 3 canales cada una)\n",
    "    policy_net = CNNActorCritic(in_channels=2, num_actions=2)\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    if checkpoint is not None and os.path.exists(checkpoint):\n",
    "        print(f\"Cargando modelo desde {checkpoint}\")\n",
    "        model = torch.load(checkpoint)\n",
    "        policy_net.load_state_dict(model['model_state_dict'])\n",
    "        optimizer.load_state_dict(model['optimizer_state_dict'])\n",
    "        print(f\"Modelo y optimizador cargados correctamente.\")\n",
    "    else:\n",
    "        print(\"Entrenando desde cero.\")\n",
    "\n",
    "    ppo_train(env, policy_net, optimizer,\n",
    "              epochs=10, episodes_per_epoch=100,\n",
    "              gamma=1.0, epsilon=0.2, entropy_coef=0.01)\n",
    "    \n",
    "    # Guardar modelo\n",
    "    torch.save({\n",
    "        \"model_state_dict\": policy_net.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "\n",
    "    print(\"Modelo guardado en\", checkpoint)\n",
    "\n",
    "    # Evaluación\n",
    "    policy_net.eval()\n",
    "    test_rewards = []\n",
    "    for _ in range(1000):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = policy_net(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        _, reward, done, truncated, info = env.step(action)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "    print(f\"\\nRecompensa promedio en 1000 episodios de test: {np.mean(test_rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de imágenes\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # load image\n",
    "    image = cv2.imread(image)\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # resize to 28x28\n",
    "    resized = cv2.resize(gray, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    # normalize\n",
    "    normalized = resized / 255.0\n",
    "\n",
    "    # convert to tensor\n",
    "    tensor = torch.tensor(normalized, dtype=torch.float32).unsqueeze(0)\n",
    "    #tensor = np.asarray(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_a = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/apple.jpg\")\n",
    "image_b = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/cabbage.jpg\")\n",
    "image_c = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/rock.jpg\")\n",
    "image_d = preprocess_image(\"/Users/mtazc/Documents/AI_projects/test_ppo_rl/stick.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando desde cero.\n",
      "Epoch 1/10: Loss=19.112 PolicyLoss=-4.785 ValueLoss=47.809 Entropy=0.693 AvgReward=4.70\n",
      "Epoch 2/10: Loss=22.814 PolicyLoss=-5.720 ValueLoss=57.083 Entropy=0.693 AvgReward=5.80\n",
      "Epoch 3/10: Loss=21.513 PolicyLoss=-5.405 ValueLoss=53.850 Entropy=0.692 AvgReward=5.60\n",
      "Epoch 4/10: Loss=20.493 PolicyLoss=-5.153 ValueLoss=51.307 Entropy=0.689 AvgReward=5.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/7c17qd49613309v3j7cdm95w0000gn/T/ipykernel_31629/2615604647.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  states_tensor       = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Loss=22.762 PolicyLoss=-5.847 ValueLoss=57.233 Entropy=0.681 AvgReward=6.40\n",
      "Epoch 6/10: Loss=18.319 PolicyLoss=-4.579 ValueLoss=45.810 Entropy=0.667 AvgReward=5.40\n",
      "Epoch 7/10: Loss=19.426 PolicyLoss=-5.038 ValueLoss=48.940 Entropy=0.639 AvgReward=6.20\n",
      "Epoch 8/10: Loss=18.292 PolicyLoss=-4.815 ValueLoss=46.225 Entropy=0.587 AvgReward=6.40\n",
      "Epoch 9/10: Loss=16.255 PolicyLoss=-4.195 ValueLoss=40.911 Entropy=0.506 AvgReward=6.30\n",
      "Epoch 10/10: Loss=14.694 PolicyLoss=-3.765 ValueLoss=36.927 Entropy=0.394 AvgReward=6.50\n",
      "Modelo guardado en ppo_checkpoint.pth\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 7.18\n"
     ]
    }
   ],
   "source": [
    "training_executor(image_a, image_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "Epoch 1/10: Loss=9.454 PolicyLoss=3.446 ValueLoss=12.021 Entropy=0.188 AvgReward=0.82\n",
      "Epoch 2/10: Loss=11.801 PolicyLoss=3.944 ValueLoss=15.716 Entropy=0.116 AvgReward=0.80\n",
      "Epoch 3/10: Loss=12.141 PolicyLoss=4.013 ValueLoss=16.257 Entropy=0.082 AvgReward=0.81\n",
      "Epoch 4/10: Loss=11.674 PolicyLoss=3.914 ValueLoss=15.522 Entropy=0.066 AvgReward=0.71\n",
      "Epoch 5/10: Loss=9.690 PolicyLoss=3.494 ValueLoss=12.394 Entropy=0.060 AvgReward=0.75\n",
      "Epoch 6/10: Loss=7.756 PolicyLoss=3.039 ValueLoss=9.434 Entropy=0.061 AvgReward=0.73\n",
      "Epoch 7/10: Loss=5.864 PolicyLoss=2.539 ValueLoss=6.650 Entropy=0.067 AvgReward=0.72\n",
      "Epoch 8/10: Loss=4.012 PolicyLoss=1.975 ValueLoss=4.074 Entropy=0.077 AvgReward=0.78\n",
      "Epoch 9/10: Loss=2.750 PolicyLoss=1.515 ValueLoss=2.472 Entropy=0.090 AvgReward=0.77\n",
      "Epoch 10/10: Loss=1.819 PolicyLoss=1.110 ValueLoss=1.420 Entropy=0.106 AvgReward=0.75\n",
      "Modelo guardado en ppo_checkpoint.pth\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 0.73\n"
     ]
    }
   ],
   "source": [
    "training_executor(image_c, image_d, reward_a=1.0, reward_b=1.0, prob_a=0.75, prob_b=0.25, checkpoint='ppo_checkpoint.pth', model_save_path='c_d_ppo_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde c_d_ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "Epoch 1/10: Loss=7.770 PolicyLoss=-0.775 ValueLoss=17.093 Entropy=0.170 AvgReward=2.13\n",
      "Epoch 2/10: Loss=8.935 PolicyLoss=-1.406 ValueLoss=20.685 Entropy=0.190 AvgReward=2.51\n",
      "Epoch 3/10: Loss=8.984 PolicyLoss=-1.620 ValueLoss=21.212 Entropy=0.208 AvgReward=2.54\n",
      "Epoch 4/10: Loss=7.417 PolicyLoss=-1.293 ValueLoss=17.424 Entropy=0.222 AvgReward=2.08\n",
      "Epoch 5/10: Loss=9.838 PolicyLoss=-2.029 ValueLoss=23.739 Entropy=0.233 AvgReward=2.72\n",
      "Epoch 6/10: Loss=8.183 PolicyLoss=-1.619 ValueLoss=19.608 Entropy=0.238 AvgReward=2.25\n",
      "Epoch 7/10: Loss=6.866 PolicyLoss=-1.234 ValueLoss=16.205 Entropy=0.241 AvgReward=1.83\n",
      "Epoch 8/10: Loss=10.272 PolicyLoss=-2.274 ValueLoss=25.097 Entropy=0.241 AvgReward=2.85\n",
      "Epoch 9/10: Loss=8.901 PolicyLoss=-1.871 ValueLoss=21.550 Entropy=0.238 AvgReward=2.45\n",
      "Epoch 10/10: Loss=8.579 PolicyLoss=-1.725 ValueLoss=20.613 Entropy=0.232 AvgReward=2.32\n",
      "Modelo guardado en c_d_ppo_checkpoint.pth\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 2.41\n"
     ]
    }
   ],
   "source": [
    "training_executor(image_b, image_c, reward_a=10.0, reward_b=1.0, prob_a=0.25, prob_b=0.75, checkpoint='c_d_ppo_checkpoint.pth', model_save_path='b_c_ppo_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(image1, image2, reward1, reward2, prob1, prob2, checkpoint):\n",
    "\n",
    "    env = SingleStepImageEnv(image1, image2, rewardA=reward1, rewardB=reward2, probA=prob1, probB=prob2)\n",
    "\n",
    "    # load model\n",
    "    policy_net = CNNActorCritic(in_channels=2, num_actions=2)\n",
    "\n",
    "    if checkpoint is not None and os.path.exists(checkpoint):\n",
    "        print(f\"Cargando modelo desde {checkpoint}\")\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Modelo y optimizador cargados correctamente.\")\n",
    "    else:\n",
    "        print(\"Checkpoint no encontrado, la evaluación no se puede realizar.\")\n",
    "        return -1 \n",
    "    policy_net.eval()\n",
    "    test_rewards = []\n",
    "    for _ in range(1000):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = policy_net(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        _, reward, done, truncated, info = env.step(action)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "    print(f\"\\nRecompensa promedio en 1000 episodios de test: {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo desde b_c_ppo_checkpoint.pth\n",
      "Modelo y optimizador cargados correctamente.\n",
      "\n",
      "Recompensa promedio en 1000 episodios de test: 2.54\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(image_b, image_c, 10.0, 1.0, 0.25, 0.75, 'b_c_ppo_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# ------- Entorno SingleStepABEnv (igual que antes) -------\n",
    "class SingleStepABEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SingleStepABEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.array([0.0], dtype=np.float32)\n",
    "        self.done = False\n",
    "        info = {}\n",
    "        return self.state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episodio ya terminó. Llama reset().\")\n",
    "        if action == 0:  # A\n",
    "            reward = 10.0 if np.random.rand() < 0.75 else 0.0\n",
    "        else:            # B\n",
    "            reward = 10.0 if np.random.rand() < 0.25 else 0.0\n",
    "        self.done = True\n",
    "        return self.state, reward, self.done, False, {}\n",
    "\n",
    "# ------- Red Actor-Crítico -------\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_size)\n",
    "        self.actor = nn.Linear(hidden_size, act_dim)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "# ------- Función para calcular returns y ventajas en 1-step -------\n",
    "def compute_returns_and_advantages(rewards, values, gamma=1.0):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    for r, v in zip(rewards, values):\n",
    "        G = r  # un solo paso\n",
    "        A = G - v\n",
    "        returns.append(G)\n",
    "        advantages.append(A)\n",
    "    return returns, advantages\n",
    "\n",
    "def ppo_train(env, policy_net, optimizer, epochs=10, episodes_per_epoch=500, gamma=1.0, epsilon=0.2, entropy_coef=0.1, eps_greedy=0.1):\n",
    "    for epoch in range(epochs):\n",
    "        states, actions, rewards, log_probs_old, values_old = [], [], [], [], []\n",
    "\n",
    "        # Recolectar datos\n",
    "        for _ in range(episodes_per_epoch):\n",
    "            state, _ = env.reset()\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Forward\n",
    "            logits, value = policy_net(state_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "\n",
    "            # Epsilon-greedy\n",
    "            if np.random.rand() < eps_greedy:\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "                log_prob = dist.log_prob(torch.tensor(action))\n",
    "            else:\n",
    "                action = dist.sample().item()\n",
    "                log_prob = dist.log_prob(torch.tensor(action))\n",
    "\n",
    "            # Step\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs_old.append(log_prob.item())\n",
    "            values_old.append(value.item())\n",
    "\n",
    "        returns, advantages = compute_returns_and_advantages(rewards, values_old, gamma=gamma)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        states_tensor       = torch.tensor(states,      dtype=torch.float32)\n",
    "        actions_tensor      = torch.tensor(actions,     dtype=torch.long)\n",
    "        old_log_probs_tensor = torch.tensor(log_probs_old, dtype=torch.float32)\n",
    "        returns_tensor      = torch.tensor(returns,     dtype=torch.float32)\n",
    "        advantages_tensor   = torch.tensor(advantages,  dtype=torch.float32)\n",
    "\n",
    "        # Forward en batch\n",
    "        logits, values = policy_net(states_tensor)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions_tensor)\n",
    "\n",
    "        ratio = torch.exp(log_probs - old_log_probs_tensor)\n",
    "        ratio_clipped = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "        policy_loss_1 = ratio         * advantages_tensor\n",
    "        policy_loss_2 = ratio_clipped * advantages_tensor\n",
    "        policy_loss   = -torch.mean(torch.min(policy_loss_1, policy_loss_2))\n",
    "\n",
    "        value_loss = torch.mean((values.squeeze() - returns_tensor)**2)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        # Aumentamos el peso de la entropía (entropy_coef) -> fomenta exploración\n",
    "        loss = policy_loss + 0.5 * value_loss - entropy_coef * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}  Loss={loss.item():.3f}  PolicyLoss={policy_loss.item():.3f}  ValueLoss={value_loss.item():.3f}  Entropy={entropy.item():.3f}  AvgReward={avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = SingleStepABEnv()\n",
    "    obs_dim = env.observation_space.shape[0] # 1\n",
    "    act_dim = env.action_space.n            # 2\n",
    "\n",
    "    policy_net = ActorCriticNet(obs_dim, act_dim, hidden_size=32)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=5e-4)  # Reducimos un poco LR\n",
    "\n",
    "    ppo_train(env, policy_net, optimizer,\n",
    "              epochs=1000,           # más epochs si lo deseas\n",
    "              episodes_per_epoch=500,\n",
    "              gamma=1.0, epsilon=0.2,\n",
    "              entropy_coef=0.1,    # subimos la entropía\n",
    "              eps_greedy=0.1)      # Forzamos 10% de exploración aleatoria\n",
    "\n",
    "    # Evaluación\n",
    "    policy_net.eval()\n",
    "    test_rewards = []\n",
    "    n_test = 1000\n",
    "    for _ in range(n_test):\n",
    "        state, _ = env.reset()\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = policy_net(state_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        _, reward, _, _, _ = env.step(action)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "    print(f\"\\nEvaluación final: Recompensa media en {n_test} episodios = {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/7c17qd49613309v3j7cdm95w0000gn/T/ipykernel_6496/494195587.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  states_tensor       = torch.tensor(states,      dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000  Loss=18.416  PolicyLoss=-4.607  ValueLoss=46.181  Entropy=0.680  AvgReward=4.80\n",
      "Epoch 2/1000  Loss=18.170  PolicyLoss=-4.542  ValueLoss=45.560  Entropy=0.681  AvgReward=4.74\n",
      "Epoch 3/1000  Loss=18.077  PolicyLoss=-4.517  ValueLoss=45.325  Entropy=0.682  AvgReward=4.72\n",
      "Epoch 4/1000  Loss=17.226  PolicyLoss=-4.292  ValueLoss=43.174  Entropy=0.682  AvgReward=4.50\n",
      "Epoch 5/1000  Loss=18.195  PolicyLoss=-4.547  ValueLoss=45.622  Entropy=0.683  AvgReward=4.76\n",
      "Epoch 6/1000  Loss=17.951  PolicyLoss=-4.483  ValueLoss=45.005  Entropy=0.683  AvgReward=4.70\n",
      "Epoch 7/1000  Loss=19.219  PolicyLoss=-4.818  ValueLoss=48.211  Entropy=0.684  AvgReward=5.04\n",
      "Epoch 8/1000  Loss=17.541  PolicyLoss=-4.373  ValueLoss=43.964  Entropy=0.685  AvgReward=4.60\n",
      "Epoch 9/1000  Loss=17.675  PolicyLoss=-4.408  ValueLoss=44.304  Entropy=0.685  AvgReward=4.64\n",
      "Epoch 10/1000  Loss=17.282  PolicyLoss=-4.304  ValueLoss=43.309  Entropy=0.686  AvgReward=4.54\n",
      "Epoch 11/1000  Loss=18.770  PolicyLoss=-4.699  ValueLoss=47.075  Entropy=0.686  AvgReward=4.94\n",
      "Epoch 12/1000  Loss=17.326  PolicyLoss=-4.314  ValueLoss=43.417  Entropy=0.687  AvgReward=4.56\n",
      "Epoch 13/1000  Loss=17.910  PolicyLoss=-4.469  ValueLoss=44.896  Entropy=0.687  AvgReward=4.72\n",
      "Epoch 14/1000  Loss=19.017  PolicyLoss=-4.764  ValueLoss=47.700  Entropy=0.688  AvgReward=5.02\n",
      "Epoch 15/1000  Loss=17.802  PolicyLoss=-4.440  ValueLoss=44.620  Entropy=0.688  AvgReward=4.70\n",
      "Epoch 16/1000  Loss=18.682  PolicyLoss=-4.675  ValueLoss=46.851  Entropy=0.688  AvgReward=4.94\n",
      "Epoch 17/1000  Loss=17.992  PolicyLoss=-4.490  ValueLoss=45.103  Entropy=0.689  AvgReward=4.76\n",
      "Epoch 18/1000  Loss=19.242  PolicyLoss=-4.825  ValueLoss=48.273  Entropy=0.689  AvgReward=5.10\n",
      "Epoch 19/1000  Loss=19.224  PolicyLoss=-4.820  ValueLoss=48.226  Entropy=0.690  AvgReward=5.10\n",
      "Epoch 20/1000  Loss=17.719  PolicyLoss=-4.416  ValueLoss=44.407  Entropy=0.690  AvgReward=4.70\n",
      "Epoch 21/1000  Loss=17.331  PolicyLoss=-4.311  ValueLoss=43.422  Entropy=0.690  AvgReward=4.60\n",
      "Epoch 22/1000  Loss=19.168  PolicyLoss=-4.806  ValueLoss=48.086  Entropy=0.691  AvgReward=5.10\n",
      "Epoch 23/1000  Loss=19.150  PolicyLoss=-4.801  ValueLoss=48.040  Entropy=0.691  AvgReward=5.10\n",
      "Epoch 24/1000  Loss=17.505  PolicyLoss=-4.356  ValueLoss=43.861  Entropy=0.691  AvgReward=4.66\n",
      "Epoch 25/1000  Loss=18.375  PolicyLoss=-4.591  ValueLoss=46.070  Entropy=0.691  AvgReward=4.90\n",
      "Epoch 26/1000  Loss=18.652  PolicyLoss=-4.666  ValueLoss=46.776  Entropy=0.692  AvgReward=4.98\n",
      "Epoch 27/1000  Loss=18.561  PolicyLoss=-4.642  ValueLoss=46.543  Entropy=0.692  AvgReward=4.96\n",
      "Epoch 28/1000  Loss=18.764  PolicyLoss=-4.697  ValueLoss=47.059  Entropy=0.692  AvgReward=5.02\n",
      "Epoch 29/1000  Loss=19.774  PolicyLoss=-4.972  ValueLoss=49.630  Entropy=0.692  AvgReward=5.30\n",
      "Epoch 30/1000  Loss=18.508  PolicyLoss=-4.627  ValueLoss=46.408  Entropy=0.692  AvgReward=4.96\n",
      "Epoch 31/1000  Loss=18.929  PolicyLoss=-4.742  ValueLoss=47.481  Entropy=0.693  AvgReward=5.08\n",
      "Epoch 32/1000  Loss=16.790  PolicyLoss=-4.157  ValueLoss=42.032  Entropy=0.693  AvgReward=4.50\n",
      "Epoch 33/1000  Loss=18.235  PolicyLoss=-4.552  ValueLoss=45.714  Entropy=0.693  AvgReward=4.90\n",
      "Epoch 34/1000  Loss=17.707  PolicyLoss=-4.407  ValueLoss=44.368  Entropy=0.693  AvgReward=4.76\n",
      "Epoch 35/1000  Loss=19.876  PolicyLoss=-5.003  ValueLoss=49.897  Entropy=0.693  AvgReward=5.36\n",
      "Epoch 36/1000  Loss=17.019  PolicyLoss=-4.218  ValueLoss=42.613  Entropy=0.693  AvgReward=4.58\n",
      "Epoch 37/1000  Loss=17.004  PolicyLoss=-4.213  ValueLoss=42.572  Entropy=0.693  AvgReward=4.58\n",
      "Epoch 38/1000  Loss=18.439  PolicyLoss=-4.608  ValueLoss=46.234  Entropy=0.693  AvgReward=4.98\n",
      "Epoch 39/1000  Loss=17.915  PolicyLoss=-4.463  ValueLoss=44.894  Entropy=0.693  AvgReward=4.84\n",
      "Epoch 40/1000  Loss=17.753  PolicyLoss=-4.418  ValueLoss=44.482  Entropy=0.693  AvgReward=4.80\n",
      "Epoch 41/1000  Loss=18.965  PolicyLoss=-4.754  ValueLoss=47.576  Entropy=0.693  AvgReward=5.14\n",
      "Epoch 42/1000  Loss=19.597  PolicyLoss=-4.929  ValueLoss=49.189  Entropy=0.693  AvgReward=5.32\n",
      "Epoch 43/1000  Loss=18.424  PolicyLoss=-4.604  ValueLoss=46.195  Entropy=0.693  AvgReward=5.00\n",
      "Epoch 44/1000  Loss=18.838  PolicyLoss=-4.719  ValueLoss=47.253  Entropy=0.693  AvgReward=5.12\n",
      "Epoch 45/1000  Loss=19.036  PolicyLoss=-4.774  ValueLoss=47.758  Entropy=0.693  AvgReward=5.18\n",
      "Epoch 46/1000  Loss=19.663  PolicyLoss=-4.949  ValueLoss=49.363  Entropy=0.693  AvgReward=5.36\n",
      "Epoch 47/1000  Loss=19.644  PolicyLoss=-4.944  ValueLoss=49.314  Entropy=0.693  AvgReward=5.36\n",
      "Epoch 48/1000  Loss=18.336  PolicyLoss=-4.579  ValueLoss=45.968  Entropy=0.692  AvgReward=5.00\n",
      "Epoch 49/1000  Loss=18.389  PolicyLoss=-4.594  ValueLoss=46.105  Entropy=0.692  AvgReward=5.02\n",
      "Epoch 50/1000  Loss=18.657  PolicyLoss=-4.669  ValueLoss=46.791  Entropy=0.692  AvgReward=5.10\n",
      "Epoch 51/1000  Loss=18.282  PolicyLoss=-4.564  ValueLoss=45.832  Entropy=0.692  AvgReward=5.00\n",
      "Epoch 52/1000  Loss=18.834  PolicyLoss=-4.719  ValueLoss=47.245  Entropy=0.692  AvgReward=5.16\n",
      "Epoch 53/1000  Loss=18.958  PolicyLoss=-4.754  ValueLoss=47.563  Entropy=0.691  AvgReward=5.20\n",
      "Epoch 54/1000  Loss=18.371  PolicyLoss=-4.589  ValueLoss=46.059  Entropy=0.691  AvgReward=5.04\n",
      "Epoch 55/1000  Loss=18.070  PolicyLoss=-4.504  ValueLoss=45.287  Entropy=0.691  AvgReward=4.96\n",
      "Epoch 56/1000  Loss=20.318  PolicyLoss=-5.139  ValueLoss=51.052  Entropy=0.691  AvgReward=5.60\n",
      "Epoch 57/1000  Loss=18.813  PolicyLoss=-4.714  ValueLoss=47.192  Entropy=0.690  AvgReward=5.18\n",
      "Epoch 58/1000  Loss=18.441  PolicyLoss=-4.609  ValueLoss=46.239  Entropy=0.690  AvgReward=5.08\n",
      "Epoch 59/1000  Loss=19.903  PolicyLoss=-5.024  ValueLoss=49.993  Entropy=0.689  AvgReward=5.50\n",
      "Epoch 60/1000  Loss=18.123  PolicyLoss=-4.519  ValueLoss=45.423  Entropy=0.689  AvgReward=5.00\n",
      "Epoch 61/1000  Loss=19.652  PolicyLoss=-4.954  ValueLoss=49.350  Entropy=0.689  AvgReward=5.44\n",
      "Epoch 62/1000  Loss=19.211  PolicyLoss=-4.829  ValueLoss=48.218  Entropy=0.688  AvgReward=5.32\n",
      "Epoch 63/1000  Loss=17.720  PolicyLoss=-4.404  ValueLoss=44.385  Entropy=0.688  AvgReward=4.90\n",
      "Epoch 64/1000  Loss=19.942  PolicyLoss=-5.039  ValueLoss=50.100  Entropy=0.687  AvgReward=5.54\n",
      "Epoch 65/1000  Loss=18.734  PolicyLoss=-4.694  ValueLoss=46.993  Entropy=0.687  AvgReward=5.20\n",
      "Epoch 66/1000  Loss=19.273  PolicyLoss=-4.849  ValueLoss=48.382  Entropy=0.686  AvgReward=5.36\n",
      "Epoch 67/1000  Loss=18.627  PolicyLoss=-4.664  ValueLoss=46.718  Entropy=0.686  AvgReward=5.18\n",
      "Epoch 68/1000  Loss=19.652  PolicyLoss=-4.959  ValueLoss=49.358  Entropy=0.685  AvgReward=5.48\n",
      "Epoch 69/1000  Loss=19.910  PolicyLoss=-5.034  ValueLoss=50.023  Entropy=0.685  AvgReward=5.56\n",
      "Epoch 70/1000  Loss=18.085  PolicyLoss=-4.508  ValueLoss=45.324  Entropy=0.684  AvgReward=5.04\n",
      "Epoch 71/1000  Loss=20.769  PolicyLoss=-5.283  ValueLoss=52.241  Entropy=0.684  AvgReward=5.82\n",
      "Epoch 72/1000  Loss=18.949  PolicyLoss=-4.758  ValueLoss=47.550  Entropy=0.683  AvgReward=5.30\n",
      "Epoch 73/1000  Loss=18.584  PolicyLoss=-4.653  ValueLoss=46.610  Entropy=0.683  AvgReward=5.20\n",
      "Epoch 74/1000  Loss=18.841  PolicyLoss=-4.728  ValueLoss=47.274  Entropy=0.682  AvgReward=5.28\n",
      "Epoch 75/1000  Loss=19.992  PolicyLoss=-5.063  ValueLoss=50.247  Entropy=0.681  AvgReward=5.62\n",
      "Epoch 76/1000  Loss=18.871  PolicyLoss=-4.738  ValueLoss=47.354  Entropy=0.681  AvgReward=5.30\n",
      "Epoch 77/1000  Loss=18.166  PolicyLoss=-4.532  ValueLoss=45.532  Entropy=0.680  AvgReward=5.10\n",
      "Epoch 78/1000  Loss=19.107  PolicyLoss=-4.807  ValueLoss=47.965  Entropy=0.679  AvgReward=5.38\n",
      "Epoch 79/1000  Loss=19.156  PolicyLoss=-4.822  ValueLoss=48.092  Entropy=0.679  AvgReward=5.40\n",
      "Epoch 80/1000  Loss=19.205  PolicyLoss=-4.837  ValueLoss=48.219  Entropy=0.678  AvgReward=5.42\n",
      "Epoch 81/1000  Loss=19.390  PolicyLoss=-4.892  ValueLoss=48.698  Entropy=0.677  AvgReward=5.48\n",
      "Epoch 82/1000  Loss=19.642  PolicyLoss=-4.967  ValueLoss=49.353  Entropy=0.676  AvgReward=5.56\n",
      "Epoch 83/1000  Loss=18.465  PolicyLoss=-4.621  ValueLoss=46.308  Entropy=0.676  AvgReward=5.22\n",
      "Epoch 84/1000  Loss=20.009  PolicyLoss=-5.076  ValueLoss=50.304  Entropy=0.675  AvgReward=5.68\n",
      "Epoch 85/1000  Loss=18.767  PolicyLoss=-4.711  ValueLoss=47.090  Entropy=0.674  AvgReward=5.32\n",
      "Epoch 86/1000  Loss=18.612  PolicyLoss=-4.666  ValueLoss=46.690  Entropy=0.673  AvgReward=5.28\n",
      "Epoch 87/1000  Loss=18.728  PolicyLoss=-4.700  ValueLoss=46.992  Entropy=0.672  AvgReward=5.32\n",
      "Epoch 88/1000  Loss=18.574  PolicyLoss=-4.655  ValueLoss=46.593  Entropy=0.672  AvgReward=5.28\n",
      "Epoch 89/1000  Loss=18.690  PolicyLoss=-4.690  ValueLoss=46.894  Entropy=0.671  AvgReward=5.32\n",
      "Epoch 90/1000  Loss=19.142  PolicyLoss=-4.825  ValueLoss=48.068  Entropy=0.670  AvgReward=5.46\n",
      "Epoch 91/1000  Loss=18.988  PolicyLoss=-4.780  ValueLoss=47.669  Entropy=0.669  AvgReward=5.42\n",
      "Epoch 92/1000  Loss=19.035  PolicyLoss=-4.794  ValueLoss=47.793  Entropy=0.668  AvgReward=5.44\n",
      "Epoch 93/1000  Loss=18.681  PolicyLoss=-4.689  ValueLoss=46.873  Entropy=0.667  AvgReward=5.34\n",
      "Epoch 94/1000  Loss=18.795  PolicyLoss=-4.724  ValueLoss=47.172  Entropy=0.666  AvgReward=5.38\n",
      "Epoch 95/1000  Loss=19.444  PolicyLoss=-4.919  ValueLoss=48.858  Entropy=0.665  AvgReward=5.58\n",
      "Epoch 96/1000  Loss=20.090  PolicyLoss=-5.114  ValueLoss=50.540  Entropy=0.664  AvgReward=5.78\n",
      "Epoch 97/1000  Loss=19.536  PolicyLoss=-4.948  ValueLoss=49.101  Entropy=0.663  AvgReward=5.62\n",
      "Epoch 98/1000  Loss=18.851  PolicyLoss=-4.743  ValueLoss=47.320  Entropy=0.662  AvgReward=5.42\n",
      "Epoch 99/1000  Loss=20.092  PolicyLoss=-5.118  ValueLoss=50.551  Entropy=0.661  AvgReward=5.80\n",
      "Epoch 100/1000  Loss=20.004  PolicyLoss=-5.092  ValueLoss=50.324  Entropy=0.660  AvgReward=5.78\n",
      "Epoch 101/1000  Loss=18.725  PolicyLoss=-4.707  ValueLoss=46.996  Entropy=0.659  AvgReward=5.40\n",
      "Epoch 102/1000  Loss=19.234  PolicyLoss=-4.862  ValueLoss=48.322  Entropy=0.658  AvgReward=5.56\n",
      "Epoch 103/1000  Loss=21.323  PolicyLoss=-5.496  ValueLoss=53.770  Entropy=0.657  AvgReward=6.20\n",
      "Epoch 104/1000  Loss=20.640  PolicyLoss=-5.291  ValueLoss=51.994  Entropy=0.656  AvgReward=6.00\n",
      "Epoch 105/1000  Loss=19.303  PolicyLoss=-4.885  ValueLoss=48.507  Entropy=0.655  AvgReward=5.60\n",
      "Epoch 106/1000  Loss=19.938  PolicyLoss=-5.080  ValueLoss=50.166  Entropy=0.653  AvgReward=5.80\n",
      "Epoch 107/1000  Loss=20.439  PolicyLoss=-5.234  ValueLoss=51.478  Entropy=0.652  AvgReward=5.96\n",
      "Epoch 108/1000  Loss=19.436  PolicyLoss=-4.929  ValueLoss=48.859  Entropy=0.651  AvgReward=5.66\n",
      "Epoch 109/1000  Loss=19.806  PolicyLoss=-5.043  ValueLoss=49.828  Entropy=0.650  AvgReward=5.78\n",
      "Epoch 110/1000  Loss=19.458  PolicyLoss=-4.938  ValueLoss=48.921  Entropy=0.649  AvgReward=5.68\n",
      "Epoch 111/1000  Loss=20.542  PolicyLoss=-5.272  ValueLoss=51.758  Entropy=0.648  AvgReward=6.02\n",
      "Epoch 112/1000  Loss=20.259  PolicyLoss=-5.187  ValueLoss=51.020  Entropy=0.646  AvgReward=5.94\n",
      "Epoch 113/1000  Loss=19.263  PolicyLoss=-4.881  ValueLoss=48.418  Entropy=0.645  AvgReward=5.64\n",
      "Epoch 114/1000  Loss=19.241  PolicyLoss=-4.876  ValueLoss=48.363  Entropy=0.644  AvgReward=5.64\n",
      "Epoch 115/1000  Loss=19.995  PolicyLoss=-5.110  ValueLoss=50.339  Entropy=0.642  AvgReward=5.88\n",
      "Epoch 116/1000  Loss=19.715  PolicyLoss=-5.025  ValueLoss=49.607  Entropy=0.641  AvgReward=5.80\n",
      "Epoch 117/1000  Loss=20.207  PolicyLoss=-5.179  ValueLoss=50.901  Entropy=0.640  AvgReward=5.96\n",
      "Epoch 118/1000  Loss=19.670  PolicyLoss=-5.013  ValueLoss=49.494  Entropy=0.639  AvgReward=5.80\n",
      "Epoch 119/1000  Loss=20.097  PolicyLoss=-5.148  ValueLoss=50.616  Entropy=0.637  AvgReward=5.94\n",
      "Epoch 120/1000  Loss=18.472  PolicyLoss=-4.642  ValueLoss=46.356  Entropy=0.636  AvgReward=5.44\n",
      "Epoch 121/1000  Loss=18.708  PolicyLoss=-4.717  ValueLoss=46.976  Entropy=0.635  AvgReward=5.52\n",
      "Epoch 122/1000  Loss=18.113  PolicyLoss=-4.531  ValueLoss=45.414  Entropy=0.633  AvgReward=5.34\n",
      "Epoch 123/1000  Loss=18.539  PolicyLoss=-4.665  ValueLoss=46.536  Entropy=0.632  AvgReward=5.48\n",
      "Epoch 124/1000  Loss=18.710  PolicyLoss=-4.720  ValueLoss=46.986  Entropy=0.630  AvgReward=5.54\n",
      "Epoch 125/1000  Loss=19.960  PolicyLoss=-5.114  ValueLoss=50.274  Entropy=0.629  AvgReward=5.94\n",
      "Epoch 126/1000  Loss=19.683  PolicyLoss=-5.029  ValueLoss=49.550  Entropy=0.628  AvgReward=5.86\n",
      "Epoch 127/1000  Loss=20.420  PolicyLoss=-5.263  ValueLoss=51.492  Entropy=0.626  AvgReward=6.10\n",
      "Epoch 128/1000  Loss=20.523  PolicyLoss=-5.298  ValueLoss=51.766  Entropy=0.625  AvgReward=6.14\n",
      "Epoch 129/1000  Loss=19.490  PolicyLoss=-4.972  ValueLoss=49.049  Entropy=0.624  AvgReward=5.82\n",
      "Epoch 130/1000  Loss=19.468  PolicyLoss=-4.966  ValueLoss=48.993  Entropy=0.622  AvgReward=5.82\n",
      "Epoch 131/1000  Loss=18.880  PolicyLoss=-4.781  ValueLoss=47.446  Entropy=0.621  AvgReward=5.64\n",
      "Epoch 132/1000  Loss=18.295  PolicyLoss=-4.595  ValueLoss=45.903  Entropy=0.619  AvgReward=5.46\n",
      "Epoch 133/1000  Loss=19.339  PolicyLoss=-4.929  ValueLoss=48.660  Entropy=0.618  AvgReward=5.80\n",
      "Epoch 134/1000  Loss=19.192  PolicyLoss=-4.884  ValueLoss=48.274  Entropy=0.616  AvgReward=5.76\n",
      "Epoch 135/1000  Loss=19.918  PolicyLoss=-5.118  ValueLoss=50.196  Entropy=0.615  AvgReward=6.00\n",
      "Epoch 136/1000  Loss=19.086  PolicyLoss=-4.853  ValueLoss=48.000  Entropy=0.614  AvgReward=5.74\n",
      "Epoch 137/1000  Loss=19.561  PolicyLoss=-5.007  ValueLoss=49.259  Entropy=0.612  AvgReward=5.90\n",
      "Epoch 138/1000  Loss=18.236  PolicyLoss=-4.581  ValueLoss=45.757  Entropy=0.611  AvgReward=5.48\n",
      "Epoch 139/1000  Loss=19.145  PolicyLoss=-4.876  ValueLoss=48.163  Entropy=0.609  AvgReward=5.78\n",
      "Epoch 140/1000  Loss=20.544  PolicyLoss=-5.330  ValueLoss=51.870  Entropy=0.608  AvgReward=6.24\n",
      "Epoch 141/1000  Loss=20.150  PolicyLoss=-5.204  ValueLoss=50.829  Entropy=0.606  AvgReward=6.12\n",
      "Epoch 142/1000  Loss=19.202  PolicyLoss=-4.898  ValueLoss=48.322  Entropy=0.605  AvgReward=5.82\n",
      "Epoch 143/1000  Loss=19.917  PolicyLoss=-5.133  ValueLoss=50.221  Entropy=0.603  AvgReward=6.06\n",
      "Epoch 144/1000  Loss=19.587  PolicyLoss=-5.027  ValueLoss=49.348  Entropy=0.602  AvgReward=5.96\n",
      "Epoch 145/1000  Loss=21.094  PolicyLoss=-5.521  ValueLoss=53.351  Entropy=0.600  AvgReward=6.46\n",
      "Epoch 146/1000  Loss=19.907  PolicyLoss=-5.135  ValueLoss=50.205  Entropy=0.598  AvgReward=6.08\n",
      "Epoch 147/1000  Loss=19.700  PolicyLoss=-5.069  ValueLoss=49.658  Entropy=0.597  AvgReward=6.02\n",
      "Epoch 148/1000  Loss=19.555  PolicyLoss=-5.023  ValueLoss=49.275  Entropy=0.595  AvgReward=5.98\n",
      "Epoch 149/1000  Loss=19.956  PolicyLoss=-5.158  ValueLoss=50.346  Entropy=0.594  AvgReward=6.12\n",
      "Epoch 150/1000  Loss=19.325  PolicyLoss=-4.952  ValueLoss=48.673  Entropy=0.592  AvgReward=5.92\n",
      "Epoch 151/1000  Loss=20.573  PolicyLoss=-5.366  ValueLoss=51.996  Entropy=0.590  AvgReward=6.34\n",
      "Epoch 152/1000  Loss=20.668  PolicyLoss=-5.400  ValueLoss=52.253  Entropy=0.589  AvgReward=6.38\n",
      "Epoch 153/1000  Loss=19.919  PolicyLoss=-5.154  ValueLoss=50.262  Entropy=0.587  AvgReward=6.14\n",
      "Epoch 154/1000  Loss=20.796  PolicyLoss=-5.448  ValueLoss=52.605  Entropy=0.585  AvgReward=6.44\n",
      "Epoch 155/1000  Loss=19.689  PolicyLoss=-5.082  ValueLoss=49.657  Entropy=0.583  AvgReward=6.08\n",
      "Epoch 156/1000  Loss=20.383  PolicyLoss=-5.316  ValueLoss=51.514  Entropy=0.582  AvgReward=6.32\n",
      "Epoch 157/1000  Loss=19.998  PolicyLoss=-5.190  ValueLoss=50.492  Entropy=0.580  AvgReward=6.20\n",
      "Epoch 158/1000  Loss=18.541  PolicyLoss=-4.703  ValueLoss=46.604  Entropy=0.578  AvgReward=5.72\n",
      "Epoch 159/1000  Loss=19.769  PolicyLoss=-5.117  ValueLoss=49.888  Entropy=0.576  AvgReward=6.14\n",
      "Epoch 160/1000  Loss=19.625  PolicyLoss=-5.071  ValueLoss=49.508  Entropy=0.575  AvgReward=6.10\n",
      "Epoch 161/1000  Loss=19.304  PolicyLoss=-4.965  ValueLoss=48.654  Entropy=0.573  AvgReward=6.00\n",
      "Epoch 162/1000  Loss=18.984  PolicyLoss=-4.859  ValueLoss=47.801  Entropy=0.571  AvgReward=5.90\n",
      "Epoch 163/1000  Loss=19.670  PolicyLoss=-5.093  ValueLoss=49.640  Entropy=0.570  AvgReward=6.14\n",
      "Epoch 164/1000  Loss=18.231  PolicyLoss=-4.607  ValueLoss=45.789  Entropy=0.568  AvgReward=5.66\n",
      "Epoch 165/1000  Loss=20.797  PolicyLoss=-5.481  ValueLoss=52.670  Entropy=0.566  AvgReward=6.54\n",
      "Epoch 166/1000  Loss=19.772  PolicyLoss=-5.135  ValueLoss=49.927  Entropy=0.564  AvgReward=6.20\n",
      "Epoch 167/1000  Loss=19.923  PolicyLoss=-5.189  ValueLoss=50.336  Entropy=0.562  AvgReward=6.26\n",
      "Epoch 168/1000  Loss=20.190  PolicyLoss=-5.283  ValueLoss=51.057  Entropy=0.561  AvgReward=6.36\n",
      "Epoch 169/1000  Loss=19.697  PolicyLoss=-5.117  ValueLoss=49.739  Entropy=0.559  AvgReward=6.20\n",
      "Epoch 170/1000  Loss=18.915  PolicyLoss=-4.850  ValueLoss=47.642  Entropy=0.557  AvgReward=5.94\n",
      "Epoch 171/1000  Loss=19.821  PolicyLoss=-5.164  ValueLoss=50.081  Entropy=0.555  AvgReward=6.26\n",
      "Epoch 172/1000  Loss=20.085  PolicyLoss=-5.258  ValueLoss=50.797  Entropy=0.553  AvgReward=6.36\n",
      "Epoch 173/1000  Loss=18.208  PolicyLoss=-4.612  ValueLoss=45.750  Entropy=0.552  AvgReward=5.72\n",
      "Epoch 174/1000  Loss=20.206  PolicyLoss=-5.306  ValueLoss=51.133  Entropy=0.550  AvgReward=6.42\n",
      "Epoch 175/1000  Loss=18.797  PolicyLoss=-4.819  ValueLoss=47.343  Entropy=0.548  AvgReward=5.94\n",
      "Epoch 176/1000  Loss=19.521  PolicyLoss=-5.073  ValueLoss=49.298  Entropy=0.546  AvgReward=6.20\n",
      "Epoch 177/1000  Loss=19.095  PolicyLoss=-4.927  ValueLoss=48.152  Entropy=0.544  AvgReward=6.06\n",
      "Epoch 178/1000  Loss=20.158  PolicyLoss=-5.301  ValueLoss=51.026  Entropy=0.543  AvgReward=6.44\n",
      "Epoch 179/1000  Loss=19.617  PolicyLoss=-5.115  ValueLoss=49.572  Entropy=0.541  AvgReward=6.26\n",
      "Epoch 180/1000  Loss=19.079  PolicyLoss=-4.928  ValueLoss=48.123  Entropy=0.539  AvgReward=6.08\n",
      "Epoch 181/1000  Loss=19.680  PolicyLoss=-5.142  ValueLoss=49.752  Entropy=0.537  AvgReward=6.30\n",
      "Epoch 182/1000  Loss=19.087  PolicyLoss=-4.936  ValueLoss=48.153  Entropy=0.535  AvgReward=6.10\n",
      "Epoch 183/1000  Loss=19.742  PolicyLoss=-5.170  ValueLoss=49.930  Entropy=0.534  AvgReward=6.34\n",
      "Epoch 184/1000  Loss=19.603  PolicyLoss=-5.123  ValueLoss=49.559  Entropy=0.532  AvgReward=6.30\n",
      "Epoch 185/1000  Loss=19.521  PolicyLoss=-5.097  ValueLoss=49.342  Entropy=0.530  AvgReward=6.28\n",
      "Epoch 186/1000  Loss=20.001  PolicyLoss=-5.271  ValueLoss=50.649  Entropy=0.528  AvgReward=6.46\n",
      "Epoch 187/1000  Loss=18.908  PolicyLoss=-4.884  ValueLoss=47.691  Entropy=0.526  AvgReward=6.08\n",
      "Epoch 188/1000  Loss=19.108  PolicyLoss=-4.958  ValueLoss=48.237  Entropy=0.525  AvgReward=6.16\n",
      "Epoch 189/1000  Loss=17.911  PolicyLoss=-4.532  ValueLoss=44.989  Entropy=0.523  AvgReward=5.74\n",
      "Epoch 190/1000  Loss=19.003  PolicyLoss=-4.925  ValueLoss=47.961  Entropy=0.521  AvgReward=6.14\n",
      "Epoch 191/1000  Loss=20.257  PolicyLoss=-5.379  ValueLoss=51.376  Entropy=0.519  AvgReward=6.60\n",
      "Epoch 192/1000  Loss=20.118  PolicyLoss=-5.333  ValueLoss=51.006  Entropy=0.518  AvgReward=6.56\n",
      "Epoch 193/1000  Loss=19.482  PolicyLoss=-5.106  ValueLoss=49.280  Entropy=0.516  AvgReward=6.34\n",
      "Epoch 194/1000  Loss=18.297  PolicyLoss=-4.680  ValueLoss=46.056  Entropy=0.514  AvgReward=5.92\n",
      "Epoch 195/1000  Loss=18.989  PolicyLoss=-4.934  ValueLoss=47.949  Entropy=0.512  AvgReward=6.18\n",
      "Epoch 196/1000  Loss=20.009  PolicyLoss=-5.307  ValueLoss=50.734  Entropy=0.510  AvgReward=6.56\n",
      "Epoch 197/1000  Loss=19.433  PolicyLoss=-5.101  ValueLoss=49.169  Entropy=0.508  AvgReward=6.36\n",
      "Epoch 198/1000  Loss=19.188  PolicyLoss=-5.014  ValueLoss=48.506  Entropy=0.507  AvgReward=6.28\n",
      "Epoch 199/1000  Loss=19.108  PolicyLoss=-4.988  ValueLoss=48.293  Entropy=0.505  AvgReward=6.26\n",
      "Epoch 200/1000  Loss=18.429  PolicyLoss=-4.742  ValueLoss=46.442  Entropy=0.503  AvgReward=6.02\n",
      "Epoch 201/1000  Loss=19.871  PolicyLoss=-5.275  ValueLoss=50.393  Entropy=0.501  AvgReward=6.56\n",
      "Epoch 202/1000  Loss=19.519  PolicyLoss=-5.149  ValueLoss=49.435  Entropy=0.499  AvgReward=6.44\n",
      "Epoch 203/1000  Loss=18.736  PolicyLoss=-4.862  ValueLoss=47.295  Entropy=0.497  AvgReward=6.16\n",
      "Epoch 204/1000  Loss=19.034  PolicyLoss=-4.976  ValueLoss=48.119  Entropy=0.495  AvgReward=6.28\n",
      "Epoch 205/1000  Loss=20.353  PolicyLoss=-5.469  ValueLoss=51.744  Entropy=0.493  AvgReward=6.78\n",
      "Epoch 206/1000  Loss=19.198  PolicyLoss=-5.043  ValueLoss=48.579  Entropy=0.492  AvgReward=6.36\n",
      "Epoch 207/1000  Loss=18.529  PolicyLoss=-4.796  ValueLoss=46.748  Entropy=0.490  AvgReward=6.12\n",
      "Epoch 208/1000  Loss=19.198  PolicyLoss=-5.050  ValueLoss=48.594  Entropy=0.488  AvgReward=6.38\n",
      "Epoch 209/1000  Loss=19.492  PolicyLoss=-5.163  ValueLoss=49.407  Entropy=0.486  AvgReward=6.50\n",
      "Epoch 210/1000  Loss=18.721  PolicyLoss=-4.876  ValueLoss=47.291  Entropy=0.484  AvgReward=6.22\n",
      "Epoch 211/1000  Loss=19.755  PolicyLoss=-5.270  ValueLoss=50.147  Entropy=0.482  AvgReward=6.62\n",
      "Epoch 212/1000  Loss=19.040  PolicyLoss=-5.003  ValueLoss=48.183  Entropy=0.481  AvgReward=6.36\n",
      "Epoch 213/1000  Loss=20.332  PolicyLoss=-5.497  ValueLoss=51.753  Entropy=0.479  AvgReward=6.86\n",
      "Epoch 214/1000  Loss=20.250  PolicyLoss=-5.470  ValueLoss=51.535  Entropy=0.477  AvgReward=6.84\n",
      "Epoch 215/1000  Loss=19.013  PolicyLoss=-5.003  ValueLoss=48.128  Entropy=0.475  AvgReward=6.38\n",
      "Epoch 216/1000  Loss=19.196  PolicyLoss=-5.076  ValueLoss=48.639  Entropy=0.473  AvgReward=6.46\n",
      "Epoch 217/1000  Loss=20.004  PolicyLoss=-5.390  ValueLoss=50.881  Entropy=0.471  AvgReward=6.78\n",
      "Epoch 218/1000  Loss=18.568  PolicyLoss=-4.843  ValueLoss=46.917  Entropy=0.469  AvgReward=6.24\n",
      "Epoch 219/1000  Loss=19.010  PolicyLoss=-5.016  ValueLoss=48.146  Entropy=0.467  AvgReward=6.42\n",
      "Epoch 220/1000  Loss=19.553  PolicyLoss=-5.229  ValueLoss=49.657  Entropy=0.466  AvgReward=6.64\n",
      "Epoch 221/1000  Loss=18.594  PolicyLoss=-4.863  ValueLoss=47.007  Entropy=0.464  AvgReward=6.28\n",
      "Epoch 222/1000  Loss=20.475  PolicyLoss=-5.596  ValueLoss=52.233  Entropy=0.462  AvgReward=7.02\n",
      "Epoch 223/1000  Loss=19.467  PolicyLoss=-5.209  ValueLoss=49.444  Entropy=0.460  AvgReward=6.64\n",
      "Epoch 224/1000  Loss=19.080  PolicyLoss=-5.062  ValueLoss=48.375  Entropy=0.458  AvgReward=6.50\n",
      "Epoch 225/1000  Loss=18.797  PolicyLoss=-4.955  ValueLoss=47.595  Entropy=0.456  AvgReward=6.40\n",
      "Epoch 226/1000  Loss=19.330  PolicyLoss=-5.168  ValueLoss=49.088  Entropy=0.454  AvgReward=6.62\n",
      "Epoch 227/1000  Loss=19.861  PolicyLoss=-5.382  ValueLoss=50.576  Entropy=0.452  AvgReward=6.84\n",
      "Epoch 228/1000  Loss=18.259  PolicyLoss=-4.755  ValueLoss=46.118  Entropy=0.450  AvgReward=6.22\n",
      "Epoch 229/1000  Loss=18.840  PolicyLoss=-4.988  ValueLoss=47.746  Entropy=0.449  AvgReward=6.46\n",
      "Epoch 230/1000  Loss=18.813  PolicyLoss=-4.981  ValueLoss=47.678  Entropy=0.447  AvgReward=6.46\n",
      "Epoch 231/1000  Loss=18.635  PolicyLoss=-4.914  ValueLoss=47.188  Entropy=0.445  AvgReward=6.40\n",
      "Epoch 232/1000  Loss=19.210  PolicyLoss=-5.147  ValueLoss=48.804  Entropy=0.443  AvgReward=6.64\n",
      "Epoch 233/1000  Loss=18.782  PolicyLoss=-4.980  ValueLoss=47.613  Entropy=0.441  AvgReward=6.48\n",
      "Epoch 234/1000  Loss=18.555  PolicyLoss=-4.893  ValueLoss=46.985  Entropy=0.439  AvgReward=6.40\n",
      "Epoch 235/1000  Loss=17.982  PolicyLoss=-4.667  ValueLoss=45.384  Entropy=0.438  AvgReward=6.18\n",
      "Epoch 236/1000  Loss=19.246  PolicyLoss=-5.180  ValueLoss=48.939  Entropy=0.436  AvgReward=6.70\n",
      "Epoch 237/1000  Loss=17.536  PolicyLoss=-4.493  ValueLoss=44.145  Entropy=0.434  AvgReward=6.02\n",
      "Epoch 238/1000  Loss=18.746  PolicyLoss=-4.986  ValueLoss=47.550  Entropy=0.432  AvgReward=6.52\n",
      "Epoch 239/1000  Loss=17.981  PolicyLoss=-4.679  ValueLoss=45.406  Entropy=0.430  AvgReward=6.22\n",
      "Epoch 240/1000  Loss=18.790  PolicyLoss=-5.012  ValueLoss=47.690  Entropy=0.428  AvgReward=6.56\n",
      "Epoch 241/1000  Loss=18.420  PolicyLoss=-4.865  ValueLoss=46.657  Entropy=0.427  AvgReward=6.42\n",
      "Epoch 242/1000  Loss=19.077  PolicyLoss=-5.139  ValueLoss=48.516  Entropy=0.425  AvgReward=6.70\n",
      "Epoch 243/1000  Loss=18.805  PolicyLoss=-5.032  ValueLoss=47.759  Entropy=0.423  AvgReward=6.60\n",
      "Epoch 244/1000  Loss=18.681  PolicyLoss=-4.985  ValueLoss=47.416  Entropy=0.421  AvgReward=6.56\n",
      "Epoch 245/1000  Loss=19.282  PolicyLoss=-5.238  ValueLoss=49.124  Entropy=0.419  AvgReward=6.82\n",
      "Epoch 246/1000  Loss=19.060  PolicyLoss=-5.151  ValueLoss=48.506  Entropy=0.418  AvgReward=6.74\n",
      "Epoch 247/1000  Loss=17.829  PolicyLoss=-4.644  ValueLoss=45.030  Entropy=0.416  AvgReward=6.24\n",
      "Epoch 248/1000  Loss=18.955  PolicyLoss=-5.117  ValueLoss=48.227  Entropy=0.414  AvgReward=6.72\n",
      "Epoch 249/1000  Loss=18.066  PolicyLoss=-4.750  ValueLoss=45.714  Entropy=0.412  AvgReward=6.36\n",
      "Epoch 250/1000  Loss=19.184  PolicyLoss=-5.223  ValueLoss=48.896  Entropy=0.411  AvgReward=6.84\n",
      "Epoch 251/1000  Loss=19.154  PolicyLoss=-5.216  ValueLoss=48.823  Entropy=0.409  AvgReward=6.84\n",
      "Epoch 252/1000  Loss=18.841  PolicyLoss=-5.089  ValueLoss=47.941  Entropy=0.407  AvgReward=6.72\n",
      "Epoch 253/1000  Loss=19.143  PolicyLoss=-5.222  ValueLoss=48.811  Entropy=0.406  AvgReward=6.86\n",
      "Epoch 254/1000  Loss=18.406  PolicyLoss=-4.915  ValueLoss=46.723  Entropy=0.404  AvgReward=6.56\n",
      "Epoch 255/1000  Loss=18.050  PolicyLoss=-4.768  ValueLoss=45.716  Entropy=0.402  AvgReward=6.42\n",
      "Epoch 256/1000  Loss=18.538  PolicyLoss=-4.981  ValueLoss=47.119  Entropy=0.400  AvgReward=6.64\n",
      "Epoch 257/1000  Loss=18.697  PolicyLoss=-5.054  ValueLoss=47.581  Entropy=0.399  AvgReward=6.72\n",
      "Epoch 258/1000  Loss=18.436  PolicyLoss=-4.947  ValueLoss=46.844  Entropy=0.397  AvgReward=6.62\n",
      "Epoch 259/1000  Loss=18.872  PolicyLoss=-5.139  ValueLoss=48.102  Entropy=0.395  AvgReward=6.82\n",
      "Epoch 260/1000  Loss=18.935  PolicyLoss=-5.172  ValueLoss=48.293  Entropy=0.394  AvgReward=6.86\n",
      "Epoch 261/1000  Loss=19.505  PolicyLoss=-5.425  ValueLoss=49.938  Entropy=0.392  AvgReward=7.12\n",
      "Epoch 262/1000  Loss=18.554  PolicyLoss=-5.018  ValueLoss=47.221  Entropy=0.390  AvgReward=6.72\n",
      "Epoch 263/1000  Loss=18.387  PolicyLoss=-4.951  ValueLoss=46.753  Entropy=0.389  AvgReward=6.66\n",
      "Epoch 264/1000  Loss=17.948  PolicyLoss=-4.763  ValueLoss=45.500  Entropy=0.387  AvgReward=6.48\n",
      "Epoch 265/1000  Loss=19.423  PolicyLoss=-5.416  ValueLoss=49.755  Entropy=0.385  AvgReward=7.14\n",
      "Epoch 266/1000  Loss=18.529  PolicyLoss=-5.029  ValueLoss=47.192  Entropy=0.383  AvgReward=6.76\n",
      "Epoch 267/1000  Loss=18.364  PolicyLoss=-4.962  ValueLoss=46.727  Entropy=0.382  AvgReward=6.70\n",
      "Epoch 268/1000  Loss=18.786  PolicyLoss=-5.154  ValueLoss=47.956  Entropy=0.380  AvgReward=6.90\n",
      "Epoch 269/1000  Loss=18.666  PolicyLoss=-5.107  ValueLoss=47.621  Entropy=0.378  AvgReward=6.86\n",
      "Epoch 270/1000  Loss=18.994  PolicyLoss=-5.260  ValueLoss=48.583  Entropy=0.376  AvgReward=7.02\n",
      "Epoch 271/1000  Loss=17.624  PolicyLoss=-4.652  ValueLoss=44.626  Entropy=0.375  AvgReward=6.42\n",
      "Epoch 272/1000  Loss=18.309  PolicyLoss=-4.965  ValueLoss=46.622  Entropy=0.373  AvgReward=6.74\n",
      "Epoch 273/1000  Loss=17.970  PolicyLoss=-4.818  ValueLoss=45.648  Entropy=0.371  AvgReward=6.60\n",
      "Epoch 274/1000  Loss=17.809  PolicyLoss=-4.750  ValueLoss=45.193  Entropy=0.370  AvgReward=6.54\n",
      "Epoch 275/1000  Loss=18.311  PolicyLoss=-4.983  ValueLoss=46.661  Entropy=0.368  AvgReward=6.78\n",
      "Epoch 276/1000  Loss=17.930  PolicyLoss=-4.816  ValueLoss=45.565  Entropy=0.366  AvgReward=6.62\n",
      "Epoch 277/1000  Loss=17.640  PolicyLoss=-4.688  ValueLoss=44.730  Entropy=0.365  AvgReward=6.50\n",
      "Epoch 278/1000  Loss=17.875  PolicyLoss=-4.801  ValueLoss=45.425  Entropy=0.363  AvgReward=6.62\n",
      "Epoch 279/1000  Loss=16.848  PolicyLoss=-4.334  ValueLoss=42.435  Entropy=0.361  AvgReward=6.16\n",
      "Epoch 280/1000  Loss=17.561  PolicyLoss=-4.667  ValueLoss=44.526  Entropy=0.360  AvgReward=6.50\n",
      "Epoch 281/1000  Loss=17.448  PolicyLoss=-4.619  ValueLoss=44.207  Entropy=0.358  AvgReward=6.46\n",
      "Epoch 282/1000  Loss=17.595  PolicyLoss=-4.692  ValueLoss=44.645  Entropy=0.356  AvgReward=6.54\n",
      "Epoch 283/1000  Loss=17.783  PolicyLoss=-4.785  ValueLoss=45.206  Entropy=0.355  AvgReward=6.64\n",
      "Epoch 284/1000  Loss=17.927  PolicyLoss=-4.858  ValueLoss=45.640  Entropy=0.353  AvgReward=6.72\n",
      "Epoch 285/1000  Loss=18.027  PolicyLoss=-4.911  ValueLoss=45.945  Entropy=0.351  AvgReward=6.78\n",
      "Epoch 286/1000  Loss=17.744  PolicyLoss=-4.783  ValueLoss=45.125  Entropy=0.350  AvgReward=6.66\n",
      "Epoch 287/1000  Loss=18.352  PolicyLoss=-5.076  ValueLoss=46.925  Entropy=0.348  AvgReward=6.96\n",
      "Epoch 288/1000  Loss=17.394  PolicyLoss=-4.629  ValueLoss=44.116  Entropy=0.347  AvgReward=6.52\n",
      "Epoch 289/1000  Loss=17.494  PolicyLoss=-4.682  ValueLoss=44.421  Entropy=0.345  AvgReward=6.58\n",
      "Epoch 290/1000  Loss=18.641  PolicyLoss=-5.234  ValueLoss=47.818  Entropy=0.343  AvgReward=7.14\n",
      "Epoch 291/1000  Loss=17.691  PolicyLoss=-4.787  ValueLoss=45.025  Entropy=0.342  AvgReward=6.70\n",
      "Epoch 292/1000  Loss=17.872  PolicyLoss=-4.880  ValueLoss=45.571  Entropy=0.340  AvgReward=6.80\n",
      "Epoch 293/1000  Loss=17.429  PolicyLoss=-4.672  ValueLoss=44.270  Entropy=0.338  AvgReward=6.60\n",
      "Epoch 294/1000  Loss=18.269  PolicyLoss=-5.085  ValueLoss=46.776  Entropy=0.337  AvgReward=7.02\n",
      "Epoch 295/1000  Loss=18.198  PolicyLoss=-5.058  ValueLoss=46.578  Entropy=0.335  AvgReward=7.00\n",
      "Epoch 296/1000  Loss=17.225  PolicyLoss=-4.590  ValueLoss=43.697  Entropy=0.334  AvgReward=6.54\n",
      "Epoch 297/1000  Loss=17.730  PolicyLoss=-4.843  ValueLoss=45.211  Entropy=0.332  AvgReward=6.80\n",
      "Epoch 298/1000  Loss=17.498  PolicyLoss=-4.735  ValueLoss=44.532  Entropy=0.331  AvgReward=6.70\n",
      "Epoch 299/1000  Loss=17.592  PolicyLoss=-4.788  ValueLoss=44.825  Entropy=0.329  AvgReward=6.76\n",
      "Epoch 300/1000  Loss=17.604  PolicyLoss=-4.800  ValueLoss=44.875  Entropy=0.328  AvgReward=6.78\n",
      "Epoch 301/1000  Loss=17.778  PolicyLoss=-4.893  ValueLoss=45.407  Entropy=0.326  AvgReward=6.88\n",
      "Epoch 302/1000  Loss=17.548  PolicyLoss=-4.785  ValueLoss=44.732  Entropy=0.325  AvgReward=6.78\n",
      "Epoch 303/1000  Loss=17.720  PolicyLoss=-4.878  ValueLoss=45.261  Entropy=0.323  AvgReward=6.88\n",
      "Epoch 304/1000  Loss=17.771  PolicyLoss=-4.911  ValueLoss=45.427  Entropy=0.321  AvgReward=6.92\n",
      "Epoch 305/1000  Loss=18.099  PolicyLoss=-5.083  ValueLoss=46.427  Entropy=0.320  AvgReward=7.10\n",
      "Epoch 306/1000  Loss=17.949  PolicyLoss=-5.015  ValueLoss=45.993  Entropy=0.318  AvgReward=7.04\n",
      "Epoch 307/1000  Loss=17.722  PolicyLoss=-4.908  ValueLoss=45.324  Entropy=0.317  AvgReward=6.94\n",
      "Epoch 308/1000  Loss=18.242  PolicyLoss=-5.180  ValueLoss=46.907  Entropy=0.315  AvgReward=7.22\n",
      "Epoch 309/1000  Loss=16.804  PolicyLoss=-4.453  ValueLoss=42.576  Entropy=0.314  AvgReward=6.50\n",
      "Epoch 310/1000  Loss=17.439  PolicyLoss=-4.785  ValueLoss=44.510  Entropy=0.312  AvgReward=6.84\n",
      "Epoch 311/1000  Loss=17.565  PolicyLoss=-4.857  ValueLoss=44.908  Entropy=0.311  AvgReward=6.92\n",
      "Epoch 312/1000  Loss=16.996  PolicyLoss=-4.570  ValueLoss=43.193  Entropy=0.310  AvgReward=6.64\n",
      "Epoch 313/1000  Loss=17.699  PolicyLoss=-4.942  ValueLoss=45.344  Entropy=0.308  AvgReward=7.02\n",
      "Epoch 314/1000  Loss=17.057  PolicyLoss=-4.615  ValueLoss=43.404  Entropy=0.307  AvgReward=6.70\n",
      "Epoch 315/1000  Loss=17.296  PolicyLoss=-4.747  ValueLoss=44.147  Entropy=0.305  AvgReward=6.84\n",
      "Epoch 316/1000  Loss=17.192  PolicyLoss=-4.699  ValueLoss=43.843  Entropy=0.304  AvgReward=6.80\n",
      "Epoch 317/1000  Loss=17.429  PolicyLoss=-4.832  ValueLoss=44.581  Entropy=0.302  AvgReward=6.94\n",
      "Epoch 318/1000  Loss=17.023  PolicyLoss=-4.624  ValueLoss=43.354  Entropy=0.301  AvgReward=6.74\n",
      "Epoch 319/1000  Loss=16.620  PolicyLoss=-4.416  ValueLoss=42.133  Entropy=0.299  AvgReward=6.54\n",
      "Epoch 320/1000  Loss=16.482  PolicyLoss=-4.349  ValueLoss=41.722  Entropy=0.298  AvgReward=6.48\n",
      "Epoch 321/1000  Loss=16.830  PolicyLoss=-4.541  ValueLoss=42.801  Entropy=0.296  AvgReward=6.68\n",
      "Epoch 322/1000  Loss=16.988  PolicyLoss=-4.634  ValueLoss=43.304  Entropy=0.295  AvgReward=6.78\n",
      "Epoch 323/1000  Loss=16.887  PolicyLoss=-4.586  ValueLoss=43.006  Entropy=0.293  AvgReward=6.74\n",
      "Epoch 324/1000  Loss=16.640  PolicyLoss=-4.459  ValueLoss=42.256  Entropy=0.292  AvgReward=6.62\n",
      "Epoch 325/1000  Loss=17.164  PolicyLoss=-4.751  ValueLoss=43.888  Entropy=0.291  AvgReward=6.92\n",
      "Epoch 326/1000  Loss=17.427  PolicyLoss=-4.904  ValueLoss=44.720  Entropy=0.289  AvgReward=7.08\n",
      "Epoch 327/1000  Loss=16.599  PolicyLoss=-4.456  ValueLoss=42.167  Entropy=0.288  AvgReward=6.64\n",
      "Epoch 328/1000  Loss=16.790  PolicyLoss=-4.569  ValueLoss=42.774  Entropy=0.286  AvgReward=6.76\n",
      "Epoch 329/1000  Loss=17.015  PolicyLoss=-4.701  ValueLoss=43.489  Entropy=0.285  AvgReward=6.90\n",
      "Epoch 330/1000  Loss=18.063  PolicyLoss=-5.293  ValueLoss=46.770  Entropy=0.284  AvgReward=7.50\n",
      "Epoch 331/1000  Loss=17.602  PolicyLoss=-5.046  ValueLoss=45.351  Entropy=0.282  AvgReward=7.26\n",
      "Epoch 332/1000  Loss=17.179  PolicyLoss=-4.818  ValueLoss=44.051  Entropy=0.281  AvgReward=7.04\n",
      "Epoch 333/1000  Loss=16.831  PolicyLoss=-4.630  ValueLoss=42.979  Entropy=0.280  AvgReward=6.86\n",
      "Epoch 334/1000  Loss=16.944  PolicyLoss=-4.702  ValueLoss=43.349  Entropy=0.278  AvgReward=6.94\n",
      "Epoch 335/1000  Loss=16.916  PolicyLoss=-4.695  ValueLoss=43.276  Entropy=0.277  AvgReward=6.94\n",
      "Epoch 336/1000  Loss=16.258  PolicyLoss=-4.327  ValueLoss=41.225  Entropy=0.276  AvgReward=6.58\n",
      "Epoch 337/1000  Loss=17.172  PolicyLoss=-4.859  ValueLoss=44.117  Entropy=0.274  AvgReward=7.12\n",
      "Epoch 338/1000  Loss=16.796  PolicyLoss=-4.651  ValueLoss=42.949  Entropy=0.273  AvgReward=6.92\n",
      "Epoch 339/1000  Loss=16.078  PolicyLoss=-4.244  ValueLoss=40.697  Entropy=0.272  AvgReward=6.52\n",
      "Epoch 340/1000  Loss=16.980  PolicyLoss=-4.776  ValueLoss=43.565  Entropy=0.270  AvgReward=7.06\n",
      "Epoch 341/1000  Loss=16.506  PolicyLoss=-4.508  ValueLoss=42.083  Entropy=0.269  AvgReward=6.80\n",
      "Epoch 342/1000  Loss=16.445  PolicyLoss=-4.480  ValueLoss=41.905  Entropy=0.268  AvgReward=6.78\n",
      "Epoch 343/1000  Loss=16.723  PolicyLoss=-4.653  ValueLoss=42.805  Entropy=0.267  AvgReward=6.96\n",
      "Epoch 344/1000  Loss=16.729  PolicyLoss=-4.665  ValueLoss=42.841  Entropy=0.265  AvgReward=6.98\n",
      "Epoch 345/1000  Loss=16.533  PolicyLoss=-4.557  ValueLoss=42.233  Entropy=0.264  AvgReward=6.88\n",
      "Epoch 346/1000  Loss=16.572  PolicyLoss=-4.589  ValueLoss=42.376  Entropy=0.263  AvgReward=6.92\n",
      "Epoch 347/1000  Loss=16.279  PolicyLoss=-4.422  ValueLoss=41.453  Entropy=0.262  AvgReward=6.76\n",
      "Epoch 348/1000  Loss=16.352  PolicyLoss=-4.474  ValueLoss=41.703  Entropy=0.260  AvgReward=6.82\n",
      "Epoch 349/1000  Loss=16.555  PolicyLoss=-4.606  ValueLoss=42.375  Entropy=0.259  AvgReward=6.96\n",
      "Epoch 350/1000  Loss=16.823  PolicyLoss=-4.778  ValueLoss=43.253  Entropy=0.258  AvgReward=7.14\n",
      "Epoch 351/1000  Loss=16.337  PolicyLoss=-4.491  ValueLoss=41.706  Entropy=0.257  AvgReward=6.86\n",
      "Epoch 352/1000  Loss=16.407  PolicyLoss=-4.543  ValueLoss=41.951  Entropy=0.256  AvgReward=6.92\n",
      "Epoch 353/1000  Loss=16.380  PolicyLoss=-4.535  ValueLoss=41.880  Entropy=0.254  AvgReward=6.92\n",
      "Epoch 354/1000  Loss=16.802  PolicyLoss=-4.807  ValueLoss=43.269  Entropy=0.253  AvgReward=7.20\n",
      "Epoch 355/1000  Loss=16.549  PolicyLoss=-4.659  ValueLoss=42.466  Entropy=0.252  AvgReward=7.06\n",
      "Epoch 356/1000  Loss=16.456  PolicyLoss=-4.612  ValueLoss=42.186  Entropy=0.251  AvgReward=7.02\n",
      "Epoch 357/1000  Loss=16.586  PolicyLoss=-4.704  ValueLoss=42.630  Entropy=0.250  AvgReward=7.12\n",
      "Epoch 358/1000  Loss=16.021  PolicyLoss=-4.356  ValueLoss=40.804  Entropy=0.249  AvgReward=6.78\n",
      "Epoch 359/1000  Loss=16.277  PolicyLoss=-4.528  ValueLoss=41.660  Entropy=0.247  AvgReward=6.96\n",
      "Epoch 360/1000  Loss=16.125  PolicyLoss=-4.440  ValueLoss=41.179  Entropy=0.246  AvgReward=6.88\n",
      "Epoch 361/1000  Loss=16.346  PolicyLoss=-4.592  ValueLoss=41.925  Entropy=0.245  AvgReward=7.04\n",
      "Epoch 362/1000  Loss=16.102  PolicyLoss=-4.444  ValueLoss=41.141  Entropy=0.244  AvgReward=6.90\n",
      "Epoch 363/1000  Loss=16.013  PolicyLoss=-4.396  ValueLoss=40.868  Entropy=0.243  AvgReward=6.86\n",
      "Epoch 364/1000  Loss=15.803  PolicyLoss=-4.268  ValueLoss=40.192  Entropy=0.242  AvgReward=6.74\n",
      "Epoch 365/1000  Loss=16.325  PolicyLoss=-4.621  ValueLoss=41.939  Entropy=0.240  AvgReward=7.10\n",
      "Epoch 366/1000  Loss=16.418  PolicyLoss=-4.693  ValueLoss=42.269  Entropy=0.239  AvgReward=7.18\n",
      "Epoch 367/1000  Loss=15.997  PolicyLoss=-4.425  ValueLoss=40.892  Entropy=0.238  AvgReward=6.92\n",
      "Epoch 368/1000  Loss=16.330  PolicyLoss=-4.657  ValueLoss=42.021  Entropy=0.237  AvgReward=7.16\n",
      "Epoch 369/1000  Loss=15.795  PolicyLoss=-4.309  ValueLoss=40.254  Entropy=0.236  AvgReward=6.82\n",
      "Epoch 370/1000  Loss=16.035  PolicyLoss=-4.481  ValueLoss=41.079  Entropy=0.235  AvgReward=7.00\n",
      "Epoch 371/1000  Loss=16.302  PolicyLoss=-4.673  ValueLoss=41.997  Entropy=0.233  AvgReward=7.20\n",
      "Epoch 372/1000  Loss=15.922  PolicyLoss=-4.425  ValueLoss=40.740  Entropy=0.232  AvgReward=6.96\n",
      "Epoch 373/1000  Loss=15.457  PolicyLoss=-4.117  ValueLoss=39.195  Entropy=0.231  AvgReward=6.66\n",
      "Epoch 374/1000  Loss=16.041  PolicyLoss=-4.529  ValueLoss=41.187  Entropy=0.230  AvgReward=7.08\n",
      "Epoch 375/1000  Loss=15.927  PolicyLoss=-4.461  ValueLoss=40.822  Entropy=0.229  AvgReward=7.02\n",
      "Epoch 376/1000  Loss=16.072  PolicyLoss=-4.573  ValueLoss=41.336  Entropy=0.228  AvgReward=7.14\n",
      "Epoch 377/1000  Loss=15.587  PolicyLoss=-4.245  ValueLoss=39.711  Entropy=0.227  AvgReward=6.82\n",
      "Epoch 378/1000  Loss=16.157  PolicyLoss=-4.657  ValueLoss=41.674  Entropy=0.226  AvgReward=7.24\n",
      "Epoch 379/1000  Loss=15.818  PolicyLoss=-4.429  ValueLoss=40.539  Entropy=0.225  AvgReward=7.02\n",
      "Epoch 380/1000  Loss=16.211  PolicyLoss=-4.721  ValueLoss=41.909  Entropy=0.224  AvgReward=7.32\n",
      "Epoch 381/1000  Loss=15.707  PolicyLoss=-4.373  ValueLoss=40.206  Entropy=0.223  AvgReward=6.98\n",
      "Epoch 382/1000  Loss=15.902  PolicyLoss=-4.525  ValueLoss=40.899  Entropy=0.222  AvgReward=7.14\n",
      "Epoch 383/1000  Loss=15.295  PolicyLoss=-4.097  ValueLoss=38.829  Entropy=0.221  AvgReward=6.72\n",
      "Epoch 384/1000  Loss=15.408  PolicyLoss=-4.189  ValueLoss=39.237  Entropy=0.219  AvgReward=6.82\n",
      "Epoch 385/1000  Loss=16.145  PolicyLoss=-4.741  ValueLoss=41.815  Entropy=0.218  AvgReward=7.38\n",
      "Epoch 386/1000  Loss=15.140  PolicyLoss=-4.013  ValueLoss=38.350  Entropy=0.217  AvgReward=6.66\n",
      "Epoch 387/1000  Loss=15.735  PolicyLoss=-4.465  ValueLoss=40.444  Entropy=0.216  AvgReward=7.12\n",
      "Epoch 388/1000  Loss=15.173  PolicyLoss=-4.057  ValueLoss=38.503  Entropy=0.215  AvgReward=6.72\n",
      "Epoch 389/1000  Loss=15.281  PolicyLoss=-4.149  ValueLoss=38.904  Entropy=0.214  AvgReward=6.82\n",
      "Epoch 390/1000  Loss=15.864  PolicyLoss=-4.601  ValueLoss=40.974  Entropy=0.213  AvgReward=7.28\n",
      "Epoch 391/1000  Loss=14.995  PolicyLoss=-3.953  ValueLoss=37.939  Entropy=0.212  AvgReward=6.64\n",
      "Epoch 392/1000  Loss=15.416  PolicyLoss=-4.285  ValueLoss=39.444  Entropy=0.211  AvgReward=6.98\n",
      "Epoch 393/1000  Loss=15.364  PolicyLoss=-4.257  ValueLoss=39.284  Entropy=0.210  AvgReward=6.96\n",
      "Epoch 394/1000  Loss=15.467  PolicyLoss=-4.350  ValueLoss=39.675  Entropy=0.209  AvgReward=7.06\n",
      "Epoch 395/1000  Loss=15.569  PolicyLoss=-4.442  ValueLoss=40.062  Entropy=0.208  AvgReward=7.16\n",
      "Epoch 396/1000  Loss=15.363  PolicyLoss=-4.294  ValueLoss=39.354  Entropy=0.207  AvgReward=7.02\n",
      "Epoch 397/1000  Loss=15.539  PolicyLoss=-4.446  ValueLoss=40.011  Entropy=0.206  AvgReward=7.18\n",
      "Epoch 398/1000  Loss=15.361  PolicyLoss=-4.318  ValueLoss=39.397  Entropy=0.205  AvgReward=7.06\n",
      "Epoch 399/1000  Loss=15.309  PolicyLoss=-4.289  ValueLoss=39.238  Entropy=0.204  AvgReward=7.04\n",
      "Epoch 400/1000  Loss=15.308  PolicyLoss=-4.301  ValueLoss=39.259  Entropy=0.203  AvgReward=7.06\n",
      "Epoch 401/1000  Loss=14.714  PolicyLoss=-3.833  ValueLoss=37.135  Entropy=0.202  AvgReward=6.60\n",
      "Epoch 402/1000  Loss=14.936  PolicyLoss=-4.025  ValueLoss=37.964  Entropy=0.201  AvgReward=6.80\n",
      "Epoch 403/1000  Loss=15.399  PolicyLoss=-4.417  ValueLoss=39.674  Entropy=0.200  AvgReward=7.20\n",
      "Epoch 404/1000  Loss=15.082  PolicyLoss=-4.169  ValueLoss=38.543  Entropy=0.199  AvgReward=6.96\n",
      "Epoch 405/1000  Loss=15.345  PolicyLoss=-4.401  ValueLoss=39.532  Entropy=0.198  AvgReward=7.20\n",
      "Epoch 406/1000  Loss=14.960  PolicyLoss=-4.093  ValueLoss=38.146  Entropy=0.197  AvgReward=6.90\n",
      "Epoch 407/1000  Loss=15.030  PolicyLoss=-4.165  ValueLoss=38.430  Entropy=0.196  AvgReward=6.98\n",
      "Epoch 408/1000  Loss=14.816  PolicyLoss=-3.997  ValueLoss=37.666  Entropy=0.195  AvgReward=6.82\n",
      "Epoch 409/1000  Loss=14.980  PolicyLoss=-4.149  ValueLoss=38.297  Entropy=0.194  AvgReward=6.98\n",
      "Epoch 410/1000  Loss=14.838  PolicyLoss=-4.041  ValueLoss=37.798  Entropy=0.193  AvgReward=6.88\n",
      "Epoch 411/1000  Loss=15.206  PolicyLoss=-4.373  ValueLoss=39.198  Entropy=0.192  AvgReward=7.22\n",
      "Epoch 412/1000  Loss=15.225  PolicyLoss=-4.405  ValueLoss=39.300  Entropy=0.191  AvgReward=7.26\n",
      "Epoch 413/1000  Loss=14.948  PolicyLoss=-4.177  ValueLoss=38.288  Entropy=0.190  AvgReward=7.04\n",
      "Epoch 414/1000  Loss=14.742  PolicyLoss=-4.009  ValueLoss=37.540  Entropy=0.189  AvgReward=6.88\n",
      "Epoch 415/1000  Loss=15.278  PolicyLoss=-4.501  ValueLoss=39.597  Entropy=0.188  AvgReward=7.38\n",
      "Epoch 416/1000  Loss=14.582  PolicyLoss=-3.893  ValueLoss=36.988  Entropy=0.188  AvgReward=6.78\n",
      "Epoch 417/1000  Loss=14.647  PolicyLoss=-3.965  ValueLoss=37.262  Entropy=0.187  AvgReward=6.86\n",
      "Epoch 418/1000  Loss=14.953  PolicyLoss=-4.257  ValueLoss=38.457  Entropy=0.186  AvgReward=7.16\n",
      "Epoch 419/1000  Loss=14.578  PolicyLoss=-3.929  ValueLoss=37.051  Entropy=0.185  AvgReward=6.84\n",
      "Epoch 420/1000  Loss=14.166  PolicyLoss=-3.561  ValueLoss=35.490  Entropy=0.184  AvgReward=6.48\n",
      "Epoch 421/1000  Loss=14.940  PolicyLoss=-4.293  ValueLoss=38.502  Entropy=0.183  AvgReward=7.22\n",
      "Epoch 422/1000  Loss=14.530  PolicyLoss=-3.925  ValueLoss=36.946  Entropy=0.182  AvgReward=6.86\n",
      "Epoch 423/1000  Loss=14.972  PolicyLoss=-4.357  ValueLoss=38.694  Entropy=0.181  AvgReward=7.30\n",
      "Epoch 424/1000  Loss=14.714  PolicyLoss=-4.129  ValueLoss=37.723  Entropy=0.180  AvgReward=7.08\n",
      "Epoch 425/1000  Loss=14.918  PolicyLoss=-4.341  ValueLoss=38.554  Entropy=0.179  AvgReward=7.30\n",
      "Epoch 426/1000  Loss=14.230  PolicyLoss=-3.693  ValueLoss=35.882  Entropy=0.179  AvgReward=6.66\n",
      "Epoch 427/1000  Loss=14.270  PolicyLoss=-3.745  ValueLoss=36.066  Entropy=0.178  AvgReward=6.72\n",
      "Epoch 428/1000  Loss=14.533  PolicyLoss=-4.017  ValueLoss=37.136  Entropy=0.177  AvgReward=7.00\n",
      "Epoch 429/1000  Loss=14.348  PolicyLoss=-3.849  ValueLoss=36.429  Entropy=0.176  AvgReward=6.84\n",
      "Epoch 430/1000  Loss=14.305  PolicyLoss=-3.821  ValueLoss=36.288  Entropy=0.175  AvgReward=6.82\n",
      "Epoch 431/1000  Loss=14.561  PolicyLoss=-4.093  ValueLoss=37.343  Entropy=0.174  AvgReward=7.10\n",
      "Epoch 432/1000  Loss=14.379  PolicyLoss=-3.925  ValueLoss=36.643  Entropy=0.174  AvgReward=6.94\n",
      "Epoch 433/1000  Loss=14.532  PolicyLoss=-4.097  ValueLoss=37.292  Entropy=0.173  AvgReward=7.12\n",
      "Epoch 434/1000  Loss=14.779  PolicyLoss=-4.369  ValueLoss=38.330  Entropy=0.172  AvgReward=7.40\n",
      "Epoch 435/1000  Loss=14.482  PolicyLoss=-4.081  ValueLoss=37.161  Entropy=0.171  AvgReward=7.12\n",
      "Epoch 436/1000  Loss=14.343  PolicyLoss=-3.953  ValueLoss=36.627  Entropy=0.170  AvgReward=7.00\n",
      "Epoch 437/1000  Loss=14.282  PolicyLoss=-3.905  ValueLoss=36.407  Entropy=0.169  AvgReward=6.96\n",
      "Epoch 438/1000  Loss=14.258  PolicyLoss=-3.897  ValueLoss=36.344  Entropy=0.168  AvgReward=6.96\n",
      "Epoch 439/1000  Loss=14.309  PolicyLoss=-3.969  ValueLoss=36.590  Entropy=0.168  AvgReward=7.04\n",
      "Epoch 440/1000  Loss=14.691  PolicyLoss=-4.401  ValueLoss=38.216  Entropy=0.167  AvgReward=7.48\n",
      "Epoch 441/1000  Loss=14.170  PolicyLoss=-3.853  ValueLoss=36.079  Entropy=0.166  AvgReward=6.94\n",
      "Epoch 442/1000  Loss=14.292  PolicyLoss=-4.004  ValueLoss=36.626  Entropy=0.165  AvgReward=7.10\n",
      "Epoch 443/1000  Loss=14.214  PolicyLoss=-3.936  ValueLoss=36.333  Entropy=0.164  AvgReward=7.04\n",
      "Epoch 444/1000  Loss=14.474  PolicyLoss=-4.248  ValueLoss=37.478  Entropy=0.163  AvgReward=7.36\n",
      "Epoch 445/1000  Loss=14.096  PolicyLoss=-3.840  ValueLoss=35.904  Entropy=0.163  AvgReward=6.96\n",
      "Epoch 446/1000  Loss=14.055  PolicyLoss=-3.812  ValueLoss=35.767  Entropy=0.162  AvgReward=6.94\n",
      "Epoch 447/1000  Loss=14.119  PolicyLoss=-3.904  ValueLoss=36.077  Entropy=0.161  AvgReward=7.04\n",
      "Epoch 448/1000  Loss=14.232  PolicyLoss=-4.056  ValueLoss=36.608  Entropy=0.160  AvgReward=7.20\n",
      "Epoch 449/1000  Loss=13.970  PolicyLoss=-3.767  ValueLoss=35.507  Entropy=0.159  AvgReward=6.92\n",
      "Epoch 450/1000  Loss=13.897  PolicyLoss=-3.699  ValueLoss=35.225  Entropy=0.159  AvgReward=6.86\n",
      "Epoch 451/1000  Loss=13.942  PolicyLoss=-3.771  ValueLoss=35.458  Entropy=0.158  AvgReward=6.94\n",
      "Epoch 452/1000  Loss=13.739  PolicyLoss=-3.543  ValueLoss=34.595  Entropy=0.157  AvgReward=6.72\n",
      "Epoch 453/1000  Loss=13.898  PolicyLoss=-3.755  ValueLoss=35.337  Entropy=0.156  AvgReward=6.94\n",
      "Epoch 454/1000  Loss=13.876  PolicyLoss=-3.747  ValueLoss=35.277  Entropy=0.155  AvgReward=6.94\n",
      "Epoch 455/1000  Loss=13.966  PolicyLoss=-3.879  ValueLoss=35.721  Entropy=0.155  AvgReward=7.08\n",
      "Epoch 456/1000  Loss=13.785  PolicyLoss=-3.671  ValueLoss=34.942  Entropy=0.154  AvgReward=6.88\n",
      "Epoch 457/1000  Loss=13.983  PolicyLoss=-3.943  ValueLoss=35.882  Entropy=0.153  AvgReward=7.16\n",
      "Epoch 458/1000  Loss=13.913  PolicyLoss=-3.875  ValueLoss=35.606  Entropy=0.152  AvgReward=7.10\n",
      "Epoch 459/1000  Loss=13.859  PolicyLoss=-3.827  ValueLoss=35.403  Entropy=0.151  AvgReward=7.06\n",
      "Epoch 460/1000  Loss=13.821  PolicyLoss=-3.799  ValueLoss=35.271  Entropy=0.151  AvgReward=7.04\n",
      "Epoch 461/1000  Loss=13.754  PolicyLoss=-3.731  ValueLoss=35.000  Entropy=0.150  AvgReward=6.98\n",
      "Epoch 462/1000  Loss=14.133  PolicyLoss=-4.263  ValueLoss=36.822  Entropy=0.149  AvgReward=7.52\n",
      "Epoch 463/1000  Loss=13.710  PolicyLoss=-3.715  ValueLoss=34.879  Entropy=0.148  AvgReward=6.98\n",
      "Epoch 464/1000  Loss=13.833  PolicyLoss=-3.907  ValueLoss=35.509  Entropy=0.148  AvgReward=7.18\n",
      "Epoch 465/1000  Loss=14.068  PolicyLoss=-4.258  ValueLoss=36.683  Entropy=0.147  AvgReward=7.54\n",
      "Epoch 466/1000  Loss=13.757  PolicyLoss=-3.850  ValueLoss=35.245  Entropy=0.146  AvgReward=7.14\n",
      "Epoch 467/1000  Loss=13.790  PolicyLoss=-3.922  ValueLoss=35.453  Entropy=0.145  AvgReward=7.22\n",
      "Epoch 468/1000  Loss=13.600  PolicyLoss=-3.674  ValueLoss=34.576  Entropy=0.145  AvgReward=6.98\n",
      "Epoch 469/1000  Loss=13.578  PolicyLoss=-3.665  ValueLoss=34.515  Entropy=0.144  AvgReward=6.98\n",
      "Epoch 470/1000  Loss=13.637  PolicyLoss=-3.777  ValueLoss=34.858  Entropy=0.143  AvgReward=7.10\n",
      "Epoch 471/1000  Loss=13.441  PolicyLoss=-3.509  ValueLoss=33.928  Entropy=0.143  AvgReward=6.84\n",
      "Epoch 472/1000  Loss=13.539  PolicyLoss=-3.681  ValueLoss=34.469  Entropy=0.142  AvgReward=7.02\n",
      "Epoch 473/1000  Loss=13.674  PolicyLoss=-3.913  ValueLoss=35.202  Entropy=0.141  AvgReward=7.26\n",
      "Epoch 474/1000  Loss=13.612  PolicyLoss=-3.845  ValueLoss=34.941  Entropy=0.140  AvgReward=7.20\n",
      "Epoch 475/1000  Loss=13.525  PolicyLoss=-3.736  ValueLoss=34.551  Entropy=0.140  AvgReward=7.10\n",
      "Epoch 476/1000  Loss=13.490  PolicyLoss=-3.708  ValueLoss=34.424  Entropy=0.139  AvgReward=7.08\n",
      "Epoch 477/1000  Loss=13.542  PolicyLoss=-3.820  ValueLoss=34.753  Entropy=0.138  AvgReward=7.20\n",
      "Epoch 478/1000  Loss=13.568  PolicyLoss=-3.892  ValueLoss=34.948  Entropy=0.138  AvgReward=7.28\n",
      "Epoch 479/1000  Loss=13.533  PolicyLoss=-3.864  ValueLoss=34.820  Entropy=0.137  AvgReward=7.26\n",
      "Epoch 480/1000  Loss=13.473  PolicyLoss=-3.795  ValueLoss=34.564  Entropy=0.136  AvgReward=7.20\n",
      "Epoch 481/1000  Loss=13.415  PolicyLoss=-3.727  ValueLoss=34.311  Entropy=0.136  AvgReward=7.14\n",
      "Epoch 482/1000  Loss=13.439  PolicyLoss=-3.799  ValueLoss=34.502  Entropy=0.135  AvgReward=7.22\n",
      "Epoch 483/1000  Loss=13.188  PolicyLoss=-3.390  ValueLoss=33.183  Entropy=0.134  AvgReward=6.82\n",
      "Epoch 484/1000  Loss=13.348  PolicyLoss=-3.702  ValueLoss=34.127  Entropy=0.133  AvgReward=7.14\n",
      "Epoch 485/1000  Loss=13.359  PolicyLoss=-3.754  ValueLoss=34.253  Entropy=0.133  AvgReward=7.20\n",
      "Epoch 486/1000  Loss=13.293  PolicyLoss=-3.666  ValueLoss=33.944  Entropy=0.132  AvgReward=7.12\n",
      "Epoch 487/1000  Loss=13.206  PolicyLoss=-3.538  ValueLoss=33.514  Entropy=0.131  AvgReward=7.00\n",
      "Epoch 488/1000  Loss=13.292  PolicyLoss=-3.729  ValueLoss=34.068  Entropy=0.131  AvgReward=7.20\n",
      "Epoch 489/1000  Loss=13.009  PolicyLoss=-3.221  ValueLoss=32.486  Entropy=0.130  AvgReward=6.70\n",
      "Epoch 490/1000  Loss=13.052  PolicyLoss=-3.333  ValueLoss=32.797  Entropy=0.129  AvgReward=6.82\n",
      "Epoch 491/1000  Loss=13.185  PolicyLoss=-3.625  ValueLoss=33.646  Entropy=0.129  AvgReward=7.12\n",
      "Epoch 492/1000  Loss=13.164  PolicyLoss=-3.617  ValueLoss=33.587  Entropy=0.128  AvgReward=7.12\n",
      "Epoch 493/1000  Loss=13.221  PolicyLoss=-3.769  ValueLoss=34.005  Entropy=0.127  AvgReward=7.28\n",
      "Epoch 494/1000  Loss=13.179  PolicyLoss=-3.721  ValueLoss=33.825  Entropy=0.127  AvgReward=7.24\n",
      "Epoch 495/1000  Loss=13.016  PolicyLoss=-3.412  ValueLoss=32.881  Entropy=0.126  AvgReward=6.94\n",
      "Epoch 496/1000  Loss=13.237  PolicyLoss=-3.924  ValueLoss=34.349  Entropy=0.125  AvgReward=7.46\n",
      "Epoch 497/1000  Loss=13.013  PolicyLoss=-3.476  ValueLoss=33.003  Entropy=0.125  AvgReward=7.02\n",
      "Epoch 498/1000  Loss=12.984  PolicyLoss=-3.448  ValueLoss=32.888  Entropy=0.124  AvgReward=7.00\n",
      "Epoch 499/1000  Loss=12.885  PolicyLoss=-3.260  ValueLoss=32.313  Entropy=0.123  AvgReward=6.82\n",
      "Epoch 500/1000  Loss=12.927  PolicyLoss=-3.392  ValueLoss=32.661  Entropy=0.123  AvgReward=6.96\n",
      "Epoch 501/1000  Loss=12.967  PolicyLoss=-3.523  ValueLoss=33.005  Entropy=0.122  AvgReward=7.10\n",
      "Epoch 502/1000  Loss=12.971  PolicyLoss=-3.575  ValueLoss=33.118  Entropy=0.121  AvgReward=7.16\n",
      "Epoch 503/1000  Loss=13.048  PolicyLoss=-3.807  ValueLoss=33.735  Entropy=0.120  AvgReward=7.40\n",
      "Epoch 504/1000  Loss=12.978  PolicyLoss=-3.679  ValueLoss=33.337  Entropy=0.120  AvgReward=7.28\n",
      "Epoch 505/1000  Loss=12.964  PolicyLoss=-3.691  ValueLoss=33.333  Entropy=0.119  AvgReward=7.30\n",
      "Epoch 506/1000  Loss=12.980  PolicyLoss=-3.783  ValueLoss=33.549  Entropy=0.118  AvgReward=7.40\n",
      "Epoch 507/1000  Loss=12.897  PolicyLoss=-3.614  ValueLoss=33.046  Entropy=0.118  AvgReward=7.24\n",
      "Epoch 508/1000  Loss=12.846  PolicyLoss=-3.526  ValueLoss=32.767  Entropy=0.117  AvgReward=7.16\n",
      "Epoch 509/1000  Loss=12.897  PolicyLoss=-3.718  ValueLoss=33.252  Entropy=0.116  AvgReward=7.36\n",
      "Epoch 510/1000  Loss=12.846  PolicyLoss=-3.629  ValueLoss=32.974  Entropy=0.116  AvgReward=7.28\n",
      "Epoch 511/1000  Loss=12.722  PolicyLoss=-3.321  ValueLoss=32.109  Entropy=0.115  AvgReward=6.98\n",
      "Epoch 512/1000  Loss=12.736  PolicyLoss=-3.413  ValueLoss=32.320  Entropy=0.114  AvgReward=7.08\n",
      "Epoch 513/1000  Loss=12.710  PolicyLoss=-3.384  ValueLoss=32.211  Entropy=0.114  AvgReward=7.06\n",
      "Epoch 514/1000  Loss=12.791  PolicyLoss=-3.696  ValueLoss=32.998  Entropy=0.113  AvgReward=7.38\n",
      "Epoch 515/1000  Loss=12.652  PolicyLoss=-3.308  ValueLoss=31.942  Entropy=0.112  AvgReward=7.00\n",
      "Epoch 516/1000  Loss=12.729  PolicyLoss=-3.620  ValueLoss=32.720  Entropy=0.112  AvgReward=7.32\n",
      "Epoch 517/1000  Loss=12.655  PolicyLoss=-3.431  ValueLoss=32.195  Entropy=0.111  AvgReward=7.14\n",
      "Epoch 518/1000  Loss=12.652  PolicyLoss=-3.483  ValueLoss=32.292  Entropy=0.111  AvgReward=7.20\n",
      "Epoch 519/1000  Loss=12.533  PolicyLoss=-3.115  ValueLoss=31.317  Entropy=0.110  AvgReward=6.84\n",
      "Epoch 520/1000  Loss=12.563  PolicyLoss=-3.287  ValueLoss=31.722  Entropy=0.110  AvgReward=7.02\n",
      "Epoch 521/1000  Loss=12.581  PolicyLoss=-3.419  ValueLoss=32.021  Entropy=0.109  AvgReward=7.16\n",
      "Epoch 522/1000  Loss=12.486  PolicyLoss=-3.110  ValueLoss=31.215  Entropy=0.108  AvgReward=6.86\n",
      "Epoch 523/1000  Loss=12.489  PolicyLoss=-3.182  ValueLoss=31.363  Entropy=0.108  AvgReward=6.94\n",
      "Epoch 524/1000  Loss=12.556  PolicyLoss=-3.534  ValueLoss=32.201  Entropy=0.107  AvgReward=7.30\n",
      "Epoch 525/1000  Loss=12.503  PolicyLoss=-3.386  ValueLoss=31.800  Entropy=0.107  AvgReward=7.16\n",
      "Epoch 526/1000  Loss=12.475  PolicyLoss=-3.338  ValueLoss=31.648  Entropy=0.106  AvgReward=7.12\n",
      "Epoch 527/1000  Loss=12.444  PolicyLoss=-3.270  ValueLoss=31.449  Entropy=0.105  AvgReward=7.06\n",
      "Epoch 528/1000  Loss=12.454  PolicyLoss=-3.402  ValueLoss=31.733  Entropy=0.105  AvgReward=7.20\n",
      "Epoch 529/1000  Loss=12.419  PolicyLoss=-3.314  ValueLoss=31.486  Entropy=0.104  AvgReward=7.12\n",
      "Epoch 530/1000  Loss=12.393  PolicyLoss=-3.266  ValueLoss=31.338  Entropy=0.104  AvgReward=7.08\n",
      "Epoch 531/1000  Loss=12.432  PolicyLoss=-3.578  ValueLoss=32.039  Entropy=0.103  AvgReward=7.40\n",
      "Epoch 532/1000  Loss=12.417  PolicyLoss=-3.609  ValueLoss=32.074  Entropy=0.103  AvgReward=7.44\n",
      "Epoch 533/1000  Loss=12.406  PolicyLoss=-3.661  ValueLoss=32.154  Entropy=0.102  AvgReward=7.50\n",
      "Epoch 534/1000  Loss=12.350  PolicyLoss=-3.433  ValueLoss=31.586  Entropy=0.101  AvgReward=7.28\n",
      "Epoch 535/1000  Loss=12.292  PolicyLoss=-3.165  ValueLoss=30.934  Entropy=0.101  AvgReward=7.02\n",
      "Epoch 536/1000  Loss=12.310  PolicyLoss=-3.416  ValueLoss=31.473  Entropy=0.100  AvgReward=7.28\n",
      "Epoch 537/1000  Loss=12.306  PolicyLoss=-3.528  ValueLoss=31.687  Entropy=0.100  AvgReward=7.40\n",
      "Epoch 538/1000  Loss=12.237  PolicyLoss=-3.120  ValueLoss=30.733  Entropy=0.099  AvgReward=7.00\n",
      "Epoch 539/1000  Loss=12.208  PolicyLoss=-3.012  ValueLoss=30.459  Entropy=0.099  AvgReward=6.90\n",
      "Epoch 540/1000  Loss=12.213  PolicyLoss=-3.203  ValueLoss=30.851  Entropy=0.098  AvgReward=7.10\n",
      "Epoch 541/1000  Loss=12.172  PolicyLoss=-2.955  ValueLoss=30.273  Entropy=0.098  AvgReward=6.86\n",
      "Epoch 542/1000  Loss=12.196  PolicyLoss=-3.407  ValueLoss=31.226  Entropy=0.097  AvgReward=7.32\n",
      "Epoch 543/1000  Loss=12.189  PolicyLoss=-3.559  ValueLoss=31.516  Entropy=0.096  AvgReward=7.48\n",
      "Epoch 544/1000  Loss=12.156  PolicyLoss=-3.371  ValueLoss=31.072  Entropy=0.096  AvgReward=7.30\n",
      "Epoch 545/1000  Loss=12.149  PolicyLoss=-3.563  ValueLoss=31.442  Entropy=0.095  AvgReward=7.50\n",
      "Epoch 546/1000  Loss=12.117  PolicyLoss=-3.354  ValueLoss=30.961  Entropy=0.095  AvgReward=7.30\n",
      "Epoch 547/1000  Loss=12.095  PolicyLoss=-3.286  ValueLoss=30.780  Entropy=0.094  AvgReward=7.24\n",
      "Epoch 548/1000  Loss=12.065  PolicyLoss=-2.998  ValueLoss=30.144  Entropy=0.094  AvgReward=6.96\n",
      "Epoch 549/1000  Loss=12.045  PolicyLoss=-2.869  ValueLoss=29.848  Entropy=0.093  AvgReward=6.84\n",
      "Epoch 550/1000  Loss=12.046  PolicyLoss=-3.621  ValueLoss=31.354  Entropy=0.093  AvgReward=7.60\n",
      "Epoch 551/1000  Loss=12.020  PolicyLoss=-3.213  ValueLoss=30.484  Entropy=0.092  AvgReward=7.20\n",
      "Epoch 552/1000  Loss=12.000  PolicyLoss=-2.985  ValueLoss=29.989  Entropy=0.092  AvgReward=6.98\n",
      "Epoch 553/1000  Loss=11.983  PolicyLoss=-3.297  ValueLoss=30.578  Entropy=0.091  AvgReward=7.30\n",
      "Epoch 554/1000  Loss=11.964  PolicyLoss=-3.328  ValueLoss=30.603  Entropy=0.091  AvgReward=7.34\n",
      "Epoch 555/1000  Loss=11.947  PolicyLoss=-3.220  ValueLoss=30.352  Entropy=0.090  AvgReward=7.24\n",
      "Epoch 556/1000  Loss=11.939  PolicyLoss=-2.832  ValueLoss=29.561  Entropy=0.090  AvgReward=6.86\n",
      "Epoch 557/1000  Loss=11.903  PolicyLoss=-3.424  ValueLoss=30.671  Entropy=0.089  AvgReward=7.46\n",
      "Epoch 558/1000  Loss=11.901  PolicyLoss=-3.016  ValueLoss=29.851  Entropy=0.089  AvgReward=7.06\n",
      "Epoch 559/1000  Loss=11.891  PolicyLoss=-2.888  ValueLoss=29.574  Entropy=0.088  AvgReward=6.94\n",
      "Epoch 560/1000  Loss=11.885  PolicyLoss=-2.719  ValueLoss=29.227  Entropy=0.088  AvgReward=6.78\n",
      "Epoch 561/1000  Loss=11.850  PolicyLoss=-3.031  ValueLoss=29.780  Entropy=0.087  AvgReward=7.10\n",
      "Epoch 562/1000  Loss=11.812  PolicyLoss=-3.304  ValueLoss=30.249  Entropy=0.086  AvgReward=7.38\n",
      "Epoch 563/1000  Loss=11.806  PolicyLoss=-3.155  ValueLoss=29.939  Entropy=0.086  AvgReward=7.24\n",
      "Epoch 564/1000  Loss=11.783  PolicyLoss=-3.207  ValueLoss=29.998  Entropy=0.085  AvgReward=7.30\n",
      "Epoch 565/1000  Loss=11.745  PolicyLoss=-3.399  ValueLoss=30.306  Entropy=0.085  AvgReward=7.50\n",
      "Epoch 566/1000  Loss=11.745  PolicyLoss=-3.211  ValueLoss=29.929  Entropy=0.084  AvgReward=7.32\n",
      "Epoch 567/1000  Loss=11.715  PolicyLoss=-3.303  ValueLoss=30.053  Entropy=0.084  AvgReward=7.42\n",
      "Epoch 568/1000  Loss=11.734  PolicyLoss=-2.995  ValueLoss=29.474  Entropy=0.083  AvgReward=7.12\n",
      "Epoch 569/1000  Loss=11.715  PolicyLoss=-3.007  ValueLoss=29.460  Entropy=0.083  AvgReward=7.14\n",
      "Epoch 570/1000  Loss=11.622  PolicyLoss=-3.538  ValueLoss=30.338  Entropy=0.082  AvgReward=7.68\n",
      "Epoch 571/1000  Loss=11.724  PolicyLoss=-2.710  ValueLoss=28.886  Entropy=0.082  AvgReward=6.86\n",
      "Epoch 572/1000  Loss=11.673  PolicyLoss=-2.942  ValueLoss=29.246  Entropy=0.082  AvgReward=7.10\n",
      "Epoch 573/1000  Loss=11.657  PolicyLoss=-2.934  ValueLoss=29.198  Entropy=0.081  AvgReward=7.10\n",
      "Epoch 574/1000  Loss=11.572  PolicyLoss=-3.326  ValueLoss=29.811  Entropy=0.081  AvgReward=7.50\n",
      "Epoch 575/1000  Loss=11.600  PolicyLoss=-3.058  ValueLoss=29.332  Entropy=0.080  AvgReward=7.24\n",
      "Epoch 576/1000  Loss=11.488  PolicyLoss=-3.550  ValueLoss=30.092  Entropy=0.080  AvgReward=7.74\n",
      "Epoch 577/1000  Loss=11.587  PolicyLoss=-2.941  ValueLoss=29.072  Entropy=0.079  AvgReward=7.14\n",
      "Epoch 578/1000  Loss=11.525  PolicyLoss=-3.153  ValueLoss=29.372  Entropy=0.079  AvgReward=7.36\n",
      "Epoch 579/1000  Loss=11.477  PolicyLoss=-3.285  ValueLoss=29.540  Entropy=0.078  AvgReward=7.50\n",
      "Epoch 580/1000  Loss=11.530  PolicyLoss=-2.957  ValueLoss=28.989  Entropy=0.078  AvgReward=7.18\n",
      "Epoch 581/1000  Loss=11.523  PolicyLoss=-2.908  ValueLoss=28.879  Entropy=0.078  AvgReward=7.14\n",
      "Epoch 582/1000  Loss=11.527  PolicyLoss=-2.820  ValueLoss=28.709  Entropy=0.077  AvgReward=7.06\n",
      "Epoch 583/1000  Loss=11.418  PolicyLoss=-3.192  ValueLoss=29.235  Entropy=0.077  AvgReward=7.44\n",
      "Epoch 584/1000  Loss=11.405  PolicyLoss=-3.164  ValueLoss=29.153  Entropy=0.076  AvgReward=7.42\n",
      "Epoch 585/1000  Loss=11.461  PolicyLoss=-2.876  ValueLoss=28.689  Entropy=0.076  AvgReward=7.14\n",
      "Epoch 586/1000  Loss=11.408  PolicyLoss=-3.007  ValueLoss=28.846  Entropy=0.075  AvgReward=7.28\n",
      "Epoch 587/1000  Loss=11.481  PolicyLoss=-2.679  ValueLoss=28.336  Entropy=0.075  AvgReward=6.96\n",
      "Epoch 588/1000  Loss=11.375  PolicyLoss=-2.991  ValueLoss=28.748  Entropy=0.075  AvgReward=7.28\n",
      "Epoch 589/1000  Loss=11.401  PolicyLoss=-2.843  ValueLoss=28.503  Entropy=0.074  AvgReward=7.14\n",
      "Epoch 590/1000  Loss=11.368  PolicyLoss=-2.895  ValueLoss=28.540  Entropy=0.074  AvgReward=7.20\n",
      "Epoch 591/1000  Loss=11.434  PolicyLoss=-2.627  ValueLoss=28.137  Entropy=0.073  AvgReward=6.94\n",
      "Epoch 592/1000  Loss=11.306  PolicyLoss=-2.979  ValueLoss=28.583  Entropy=0.073  AvgReward=7.30\n",
      "Epoch 593/1000  Loss=11.336  PolicyLoss=-2.831  ValueLoss=28.348  Entropy=0.072  AvgReward=7.16\n",
      "Epoch 594/1000  Loss=11.220  PolicyLoss=-3.123  ValueLoss=28.701  Entropy=0.072  AvgReward=7.46\n",
      "Epoch 595/1000  Loss=11.259  PolicyLoss=-2.955  ValueLoss=28.441  Entropy=0.072  AvgReward=7.30\n",
      "Epoch 596/1000  Loss=11.179  PolicyLoss=-3.127  ValueLoss=28.627  Entropy=0.071  AvgReward=7.48\n",
      "Epoch 597/1000  Loss=11.263  PolicyLoss=-2.839  ValueLoss=28.218  Entropy=0.071  AvgReward=7.20\n",
      "Epoch 598/1000  Loss=11.167  PolicyLoss=-3.051  ValueLoss=28.450  Entropy=0.070  AvgReward=7.42\n",
      "Epoch 599/1000  Loss=11.234  PolicyLoss=-2.823  ValueLoss=28.127  Entropy=0.070  AvgReward=7.20\n",
      "Epoch 600/1000  Loss=11.281  PolicyLoss=-2.654  ValueLoss=27.884  Entropy=0.070  AvgReward=7.04\n",
      "Epoch 601/1000  Loss=11.291  PolicyLoss=-2.586  ValueLoss=27.769  Entropy=0.069  AvgReward=6.98\n",
      "Epoch 602/1000  Loss=11.182  PolicyLoss=-2.818  ValueLoss=28.015  Entropy=0.069  AvgReward=7.22\n",
      "Epoch 603/1000  Loss=11.160  PolicyLoss=-2.831  ValueLoss=27.994  Entropy=0.068  AvgReward=7.24\n",
      "Epoch 604/1000  Loss=11.195  PolicyLoss=-2.703  ValueLoss=27.810  Entropy=0.068  AvgReward=7.12\n",
      "Epoch 605/1000  Loss=11.114  PolicyLoss=-2.855  ValueLoss=27.951  Entropy=0.067  AvgReward=7.28\n",
      "Epoch 606/1000  Loss=11.065  PolicyLoss=-2.927  ValueLoss=27.996  Entropy=0.067  AvgReward=7.36\n",
      "Epoch 607/1000  Loss=11.164  PolicyLoss=-2.659  ValueLoss=27.659  Entropy=0.067  AvgReward=7.10\n",
      "Epoch 608/1000  Loss=11.079  PolicyLoss=-2.811  ValueLoss=27.793  Entropy=0.066  AvgReward=7.26\n",
      "Epoch 609/1000  Loss=11.138  PolicyLoss=-2.643  ValueLoss=27.575  Entropy=0.066  AvgReward=7.10\n",
      "Epoch 610/1000  Loss=11.237  PolicyLoss=-2.395  ValueLoss=27.277  Entropy=0.065  AvgReward=6.86\n",
      "Epoch 611/1000  Loss=11.235  PolicyLoss=-2.367  ValueLoss=27.219  Entropy=0.065  AvgReward=6.84\n",
      "Epoch 612/1000  Loss=11.081  PolicyLoss=-2.660  ValueLoss=27.494  Entropy=0.065  AvgReward=7.14\n",
      "Epoch 613/1000  Loss=11.234  PolicyLoss=-2.312  ValueLoss=27.105  Entropy=0.064  AvgReward=6.80\n",
      "Epoch 614/1000  Loss=11.016  PolicyLoss=-2.724  ValueLoss=27.494  Entropy=0.064  AvgReward=7.22\n",
      "Epoch 615/1000  Loss=10.902  PolicyLoss=-2.917  ValueLoss=27.651  Entropy=0.063  AvgReward=7.42\n",
      "Epoch 616/1000  Loss=11.092  PolicyLoss=-2.509  ValueLoss=27.215  Entropy=0.063  AvgReward=7.02\n",
      "Epoch 617/1000  Loss=10.936  PolicyLoss=-2.781  ValueLoss=27.446  Entropy=0.063  AvgReward=7.30\n",
      "Epoch 618/1000  Loss=11.069  PolicyLoss=-2.494  ValueLoss=27.139  Entropy=0.062  AvgReward=7.02\n",
      "Epoch 619/1000  Loss=10.866  PolicyLoss=-2.846  ValueLoss=27.436  Entropy=0.062  AvgReward=7.38\n",
      "Epoch 620/1000  Loss=10.852  PolicyLoss=-2.838  ValueLoss=27.392  Entropy=0.061  AvgReward=7.38\n",
      "Epoch 621/1000  Loss=10.991  PolicyLoss=-2.551  ValueLoss=27.096  Entropy=0.061  AvgReward=7.10\n",
      "Epoch 622/1000  Loss=11.035  PolicyLoss=-2.443  ValueLoss=26.968  Entropy=0.061  AvgReward=7.00\n",
      "Epoch 623/1000  Loss=11.069  PolicyLoss=-2.355  ValueLoss=26.861  Entropy=0.060  AvgReward=6.92\n",
      "Epoch 624/1000  Loss=10.979  PolicyLoss=-2.488  ValueLoss=26.945  Entropy=0.060  AvgReward=7.06\n",
      "Epoch 625/1000  Loss=10.701  PolicyLoss=-2.940  ValueLoss=27.294  Entropy=0.059  AvgReward=7.52\n",
      "Epoch 626/1000  Loss=10.886  PolicyLoss=-2.593  ValueLoss=26.969  Entropy=0.059  AvgReward=7.18\n",
      "Epoch 627/1000  Loss=10.779  PolicyLoss=-2.745  ValueLoss=27.059  Entropy=0.059  AvgReward=7.34\n",
      "Epoch 628/1000  Loss=10.910  PolicyLoss=-2.497  ValueLoss=26.826  Entropy=0.058  AvgReward=7.10\n",
      "Epoch 629/1000  Loss=10.728  PolicyLoss=-2.770  ValueLoss=27.006  Entropy=0.058  AvgReward=7.38\n",
      "Epoch 630/1000  Loss=10.764  PolicyLoss=-2.682  ValueLoss=26.902  Entropy=0.057  AvgReward=7.30\n",
      "Epoch 631/1000  Loss=10.738  PolicyLoss=-2.694  ValueLoss=26.876  Entropy=0.057  AvgReward=7.32\n",
      "Epoch 632/1000  Loss=10.839  PolicyLoss=-2.506  ValueLoss=26.702  Entropy=0.057  AvgReward=7.14\n",
      "Epoch 633/1000  Loss=10.622  PolicyLoss=-2.819  ValueLoss=26.893  Entropy=0.056  AvgReward=7.46\n",
      "Epoch 634/1000  Loss=10.998  PolicyLoss=-2.211  ValueLoss=26.428  Entropy=0.056  AvgReward=6.86\n",
      "Epoch 635/1000  Loss=10.752  PolicyLoss=-2.563  ValueLoss=26.641  Entropy=0.055  AvgReward=7.22\n",
      "Epoch 636/1000  Loss=10.806  PolicyLoss=-2.455  ValueLoss=26.535  Entropy=0.055  AvgReward=7.12\n",
      "Epoch 637/1000  Loss=10.513  PolicyLoss=-2.868  ValueLoss=26.773  Entropy=0.055  AvgReward=7.54\n",
      "Epoch 638/1000  Loss=10.526  PolicyLoss=-2.820  ValueLoss=26.703  Entropy=0.054  AvgReward=7.50\n",
      "Epoch 639/1000  Loss=10.690  PolicyLoss=-2.552  ValueLoss=26.496  Entropy=0.054  AvgReward=7.24\n",
      "Epoch 640/1000  Loss=10.776  PolicyLoss=-2.404  ValueLoss=26.371  Entropy=0.054  AvgReward=7.10\n",
      "Epoch 641/1000  Loss=10.667  PolicyLoss=-2.537  ValueLoss=26.417  Entropy=0.053  AvgReward=7.24\n",
      "Epoch 642/1000  Loss=10.498  PolicyLoss=-2.749  ValueLoss=26.505  Entropy=0.053  AvgReward=7.46\n",
      "Epoch 643/1000  Loss=10.729  PolicyLoss=-2.401  ValueLoss=26.271  Entropy=0.052  AvgReward=7.12\n",
      "Epoch 644/1000  Loss=10.776  PolicyLoss=-2.313  ValueLoss=26.190  Entropy=0.052  AvgReward=7.04\n",
      "Epoch 645/1000  Loss=10.458  PolicyLoss=-2.726  ValueLoss=26.378  Entropy=0.052  AvgReward=7.46\n",
      "Epoch 646/1000  Loss=10.994  PolicyLoss=-1.978  ValueLoss=25.954  Entropy=0.051  AvgReward=6.72\n",
      "Epoch 647/1000  Loss=10.657  PolicyLoss=-2.410  ValueLoss=26.144  Entropy=0.051  AvgReward=7.16\n",
      "Epoch 648/1000  Loss=10.737  PolicyLoss=-2.283  ValueLoss=26.050  Entropy=0.051  AvgReward=7.04\n",
      "Epoch 649/1000  Loss=10.789  PolicyLoss=-2.195  ValueLoss=25.978  Entropy=0.050  AvgReward=6.96\n",
      "Epoch 650/1000  Loss=10.795  PolicyLoss=-2.168  ValueLoss=25.936  Entropy=0.050  AvgReward=6.94\n",
      "Epoch 651/1000  Loss=10.693  PolicyLoss=-2.281  ValueLoss=25.957  Entropy=0.050  AvgReward=7.06\n",
      "Epoch 652/1000  Loss=10.369  PolicyLoss=-2.673  ValueLoss=26.094  Entropy=0.049  AvgReward=7.46\n",
      "Epoch 653/1000  Loss=10.659  PolicyLoss=-2.286  ValueLoss=25.898  Entropy=0.049  AvgReward=7.08\n",
      "Epoch 654/1000  Loss=10.457  PolicyLoss=-2.518  ValueLoss=25.960  Entropy=0.048  AvgReward=7.32\n",
      "Epoch 655/1000  Loss=10.462  PolicyLoss=-2.491  ValueLoss=25.915  Entropy=0.048  AvgReward=7.30\n",
      "Epoch 656/1000  Loss=10.451  PolicyLoss=-2.483  ValueLoss=25.878  Entropy=0.048  AvgReward=7.30\n",
      "Epoch 657/1000  Loss=10.571  PolicyLoss=-2.316  ValueLoss=25.784  Entropy=0.047  AvgReward=7.14\n",
      "Epoch 658/1000  Loss=10.312  PolicyLoss=-2.609  ValueLoss=25.851  Entropy=0.047  AvgReward=7.44\n",
      "Epoch 659/1000  Loss=10.250  PolicyLoss=-2.661  ValueLoss=25.831  Entropy=0.047  AvgReward=7.50\n",
      "Epoch 660/1000  Loss=10.610  PolicyLoss=-2.213  ValueLoss=25.656  Entropy=0.046  AvgReward=7.06\n",
      "Epoch 661/1000  Loss=10.498  PolicyLoss=-2.326  ValueLoss=25.657  Entropy=0.046  AvgReward=7.18\n",
      "Epoch 662/1000  Loss=10.316  PolicyLoss=-2.518  ValueLoss=25.678  Entropy=0.046  AvgReward=7.38\n",
      "Epoch 663/1000  Loss=10.113  PolicyLoss=-2.731  ValueLoss=25.697  Entropy=0.045  AvgReward=7.60\n",
      "Epoch 664/1000  Loss=10.275  PolicyLoss=-2.523  ValueLoss=25.606  Entropy=0.045  AvgReward=7.40\n",
      "Epoch 665/1000  Loss=10.246  PolicyLoss=-2.535  ValueLoss=25.572  Entropy=0.045  AvgReward=7.42\n",
      "Epoch 666/1000  Loss=10.234  PolicyLoss=-2.528  ValueLoss=25.533  Entropy=0.044  AvgReward=7.42\n",
      "Epoch 667/1000  Loss=10.241  PolicyLoss=-2.500  ValueLoss=25.490  Entropy=0.044  AvgReward=7.40\n",
      "Epoch 668/1000  Loss=10.229  PolicyLoss=-2.492  ValueLoss=25.451  Entropy=0.044  AvgReward=7.40\n",
      "Epoch 669/1000  Loss=10.236  PolicyLoss=-2.465  ValueLoss=25.410  Entropy=0.043  AvgReward=7.38\n",
      "Epoch 670/1000  Loss=10.428  PolicyLoss=-2.237  ValueLoss=25.338  Entropy=0.043  AvgReward=7.16\n",
      "Epoch 671/1000  Loss=10.139  PolicyLoss=-2.529  ValueLoss=25.345  Entropy=0.043  AvgReward=7.46\n",
      "Epoch 672/1000  Loss=10.428  PolicyLoss=-2.201  ValueLoss=25.266  Entropy=0.043  AvgReward=7.14\n",
      "Epoch 673/1000  Loss=10.153  PolicyLoss=-2.474  ValueLoss=25.263  Entropy=0.042  AvgReward=7.42\n",
      "Epoch 674/1000  Loss=10.562  PolicyLoss=-2.026  ValueLoss=25.184  Entropy=0.042  AvgReward=6.98\n",
      "Epoch 675/1000  Loss=10.227  PolicyLoss=-2.358  ValueLoss=25.180  Entropy=0.042  AvgReward=7.32\n",
      "Epoch 676/1000  Loss=10.508  PolicyLoss=-2.051  ValueLoss=25.125  Entropy=0.041  AvgReward=7.02\n",
      "Epoch 677/1000  Loss=10.148  PolicyLoss=-2.403  ValueLoss=25.111  Entropy=0.041  AvgReward=7.38\n",
      "Epoch 678/1000  Loss=10.354  PolicyLoss=-2.176  ValueLoss=25.069  Entropy=0.041  AvgReward=7.16\n",
      "Epoch 679/1000  Loss=10.207  PolicyLoss=-2.308  ValueLoss=25.038  Entropy=0.041  AvgReward=7.30\n",
      "Epoch 680/1000  Loss=10.577  PolicyLoss=-1.921  ValueLoss=25.003  Entropy=0.040  AvgReward=6.92\n",
      "Epoch 681/1000  Loss=10.409  PolicyLoss=-2.074  ValueLoss=24.973  Entropy=0.040  AvgReward=7.08\n",
      "Epoch 682/1000  Loss=10.361  PolicyLoss=-2.106  ValueLoss=24.942  Entropy=0.040  AvgReward=7.12\n",
      "Epoch 683/1000  Loss=10.230  PolicyLoss=-2.219  ValueLoss=24.906  Entropy=0.039  AvgReward=7.24\n",
      "Epoch 684/1000  Loss=10.283  PolicyLoss=-2.152  ValueLoss=24.878  Entropy=0.039  AvgReward=7.18\n",
      "Epoch 685/1000  Loss=9.943  PolicyLoss=-2.465  ValueLoss=24.824  Entropy=0.039  AvgReward=7.50\n",
      "Epoch 686/1000  Loss=10.496  PolicyLoss=-1.917  ValueLoss=24.834  Entropy=0.039  AvgReward=6.96\n",
      "Epoch 687/1000  Loss=10.300  PolicyLoss=-2.090  ValueLoss=24.788  Entropy=0.038  AvgReward=7.14\n",
      "Epoch 688/1000  Loss=10.314  PolicyLoss=-2.063  ValueLoss=24.761  Entropy=0.038  AvgReward=7.12\n",
      "Epoch 689/1000  Loss=10.477  PolicyLoss=-1.896  ValueLoss=24.752  Entropy=0.038  AvgReward=6.96\n",
      "Epoch 690/1000  Loss=10.127  PolicyLoss=-2.209  ValueLoss=24.680  Entropy=0.038  AvgReward=7.28\n",
      "Epoch 691/1000  Loss=10.356  PolicyLoss=-1.982  ValueLoss=24.683  Entropy=0.037  AvgReward=7.06\n",
      "Epoch 692/1000  Loss=10.045  PolicyLoss=-2.254  ValueLoss=24.607  Entropy=0.037  AvgReward=7.34\n",
      "Epoch 693/1000  Loss=10.277  PolicyLoss=-2.027  ValueLoss=24.616  Entropy=0.037  AvgReward=7.12\n",
      "Epoch 694/1000  Loss=10.160  PolicyLoss=-2.120  ValueLoss=24.567  Entropy=0.037  AvgReward=7.22\n",
      "Epoch 695/1000  Loss=10.130  PolicyLoss=-2.133  ValueLoss=24.533  Entropy=0.036  AvgReward=7.24\n",
      "Epoch 696/1000  Loss=10.033  PolicyLoss=-2.206  ValueLoss=24.485  Entropy=0.036  AvgReward=7.32\n",
      "Epoch 697/1000  Loss=9.733  PolicyLoss=-2.459  ValueLoss=24.391  Entropy=0.036  AvgReward=7.58\n",
      "Epoch 698/1000  Loss=10.241  PolicyLoss=-1.992  ValueLoss=24.473  Entropy=0.035  AvgReward=7.12\n",
      "Epoch 699/1000  Loss=9.757  PolicyLoss=-2.405  ValueLoss=24.331  Entropy=0.035  AvgReward=7.54\n",
      "Epoch 700/1000  Loss=10.250  PolicyLoss=-1.957  ValueLoss=24.421  Entropy=0.035  AvgReward=7.10\n",
      "Epoch 701/1000  Loss=10.128  PolicyLoss=-2.050  ValueLoss=24.363  Entropy=0.035  AvgReward=7.20\n",
      "Epoch 702/1000  Loss=10.352  PolicyLoss=-1.843  ValueLoss=24.397  Entropy=0.034  AvgReward=7.00\n",
      "Epoch 703/1000  Loss=10.230  PolicyLoss=-1.936  ValueLoss=24.338  Entropy=0.034  AvgReward=7.10\n",
      "Epoch 704/1000  Loss=10.129  PolicyLoss=-2.009  ValueLoss=24.283  Entropy=0.034  AvgReward=7.18\n",
      "Epoch 705/1000  Loss=10.382  PolicyLoss=-1.782  ValueLoss=24.334  Entropy=0.034  AvgReward=6.96\n",
      "Epoch 706/1000  Loss=10.068  PolicyLoss=-2.035  ValueLoss=24.213  Entropy=0.033  AvgReward=7.22\n",
      "Epoch 707/1000  Loss=10.347  PolicyLoss=-1.788  ValueLoss=24.277  Entropy=0.033  AvgReward=6.98\n",
      "Epoch 708/1000  Loss=9.694  PolicyLoss=-2.321  ValueLoss=24.038  Entropy=0.033  AvgReward=7.52\n",
      "Epoch 709/1000  Loss=9.854  PolicyLoss=-2.174  ValueLoss=24.063  Entropy=0.033  AvgReward=7.38\n",
      "Epoch 710/1000  Loss=10.040  PolicyLoss=-2.007  ValueLoss=24.101  Entropy=0.032  AvgReward=7.22\n",
      "Epoch 711/1000  Loss=9.813  PolicyLoss=-2.180  ValueLoss=23.994  Entropy=0.032  AvgReward=7.40\n",
      "Epoch 712/1000  Loss=10.369  PolicyLoss=-1.713  ValueLoss=24.172  Entropy=0.032  AvgReward=6.94\n",
      "Epoch 713/1000  Loss=9.600  PolicyLoss=-2.326  ValueLoss=23.858  Entropy=0.032  AvgReward=7.56\n",
      "Epoch 714/1000  Loss=9.938  PolicyLoss=-2.039  ValueLoss=23.960  Entropy=0.031  AvgReward=7.28\n",
      "Epoch 715/1000  Loss=10.180  PolicyLoss=-1.832  ValueLoss=24.031  Entropy=0.031  AvgReward=7.08\n",
      "Epoch 716/1000  Loss=9.848  PolicyLoss=-2.085  ValueLoss=23.873  Entropy=0.031  AvgReward=7.34\n",
      "Epoch 717/1000  Loss=10.118  PolicyLoss=-1.858  ValueLoss=23.959  Entropy=0.031  AvgReward=7.12\n",
      "Epoch 718/1000  Loss=9.706  PolicyLoss=-2.171  ValueLoss=23.761  Entropy=0.030  AvgReward=7.44\n",
      "Epoch 719/1000  Loss=10.336  PolicyLoss=-1.664  ValueLoss=24.006  Entropy=0.030  AvgReward=6.94\n",
      "Epoch 720/1000  Loss=10.331  PolicyLoss=-1.657  ValueLoss=23.984  Entropy=0.030  AvgReward=6.94\n",
      "Epoch 721/1000  Loss=10.146  PolicyLoss=-1.791  ValueLoss=23.880  Entropy=0.030  AvgReward=7.08\n",
      "Epoch 722/1000  Loss=9.830  PolicyLoss=-2.024  ValueLoss=23.714  Entropy=0.030  AvgReward=7.32\n",
      "Epoch 723/1000  Loss=9.979  PolicyLoss=-1.897  ValueLoss=23.759  Entropy=0.029  AvgReward=7.20\n",
      "Epoch 724/1000  Loss=10.628  PolicyLoss=-1.390  ValueLoss=24.043  Entropy=0.029  AvgReward=6.70\n",
      "Epoch 725/1000  Loss=9.678  PolicyLoss=-2.104  ValueLoss=23.569  Entropy=0.029  AvgReward=7.42\n",
      "Epoch 726/1000  Loss=9.644  PolicyLoss=-2.117  ValueLoss=23.528  Entropy=0.029  AvgReward=7.44\n",
      "Epoch 727/1000  Loss=10.089  PolicyLoss=-1.770  ValueLoss=23.724  Entropy=0.028  AvgReward=7.10\n",
      "Epoch 728/1000  Loss=9.977  PolicyLoss=-1.844  ValueLoss=23.646  Entropy=0.028  AvgReward=7.18\n",
      "Epoch 729/1000  Loss=9.917  PolicyLoss=-1.877  ValueLoss=23.594  Entropy=0.028  AvgReward=7.22\n",
      "Epoch 730/1000  Loss=9.723  PolicyLoss=-2.010  ValueLoss=23.471  Entropy=0.028  AvgReward=7.36\n",
      "Epoch 731/1000  Loss=9.689  PolicyLoss=-2.023  ValueLoss=23.430  Entropy=0.028  AvgReward=7.38\n",
      "Epoch 732/1000  Loss=10.118  PolicyLoss=-1.697  ValueLoss=23.635  Entropy=0.027  AvgReward=7.06\n",
      "Epoch 733/1000  Loss=10.141  PolicyLoss=-1.670  ValueLoss=23.627  Entropy=0.027  AvgReward=7.04\n",
      "Epoch 734/1000  Loss=9.696  PolicyLoss=-1.983  ValueLoss=23.364  Entropy=0.027  AvgReward=7.36\n",
      "Epoch 735/1000  Loss=9.966  PolicyLoss=-1.777  ValueLoss=23.490  Entropy=0.027  AvgReward=7.16\n",
      "Epoch 736/1000  Loss=9.905  PolicyLoss=-1.810  ValueLoss=23.436  Entropy=0.027  AvgReward=7.20\n",
      "Epoch 737/1000  Loss=10.319  PolicyLoss=-1.503  ValueLoss=23.650  Entropy=0.026  AvgReward=6.90\n",
      "Epoch 738/1000  Loss=9.698  PolicyLoss=-1.937  ValueLoss=23.275  Entropy=0.026  AvgReward=7.34\n",
      "Epoch 739/1000  Loss=10.256  PolicyLoss=-1.530  ValueLoss=23.578  Entropy=0.026  AvgReward=6.94\n",
      "Epoch 740/1000  Loss=9.941  PolicyLoss=-1.744  ValueLoss=23.375  Entropy=0.026  AvgReward=7.16\n",
      "Epoch 741/1000  Loss=10.193  PolicyLoss=-1.557  ValueLoss=23.505  Entropy=0.026  AvgReward=6.98\n",
      "Epoch 742/1000  Loss=9.617  PolicyLoss=-1.951  ValueLoss=23.142  Entropy=0.025  AvgReward=7.38\n",
      "Epoch 743/1000  Loss=10.071  PolicyLoss=-1.624  ValueLoss=23.395  Entropy=0.025  AvgReward=7.06\n",
      "Epoch 744/1000  Loss=9.403  PolicyLoss=-2.078  ValueLoss=22.968  Entropy=0.025  AvgReward=7.52\n",
      "Epoch 745/1000  Loss=9.773  PolicyLoss=-1.811  ValueLoss=23.174  Entropy=0.025  AvgReward=7.26\n",
      "Epoch 746/1000  Loss=9.739  PolicyLoss=-1.825  ValueLoss=23.132  Entropy=0.025  AvgReward=7.28\n",
      "Epoch 747/1000  Loss=9.441  PolicyLoss=-2.018  ValueLoss=22.924  Entropy=0.024  AvgReward=7.48\n",
      "Epoch 748/1000  Loss=10.168  PolicyLoss=-1.512  ValueLoss=23.365  Entropy=0.024  AvgReward=6.98\n",
      "Epoch 749/1000  Loss=9.487  PolicyLoss=-1.965  ValueLoss=22.908  Entropy=0.024  AvgReward=7.44\n",
      "Epoch 750/1000  Loss=9.599  PolicyLoss=-1.879  ValueLoss=22.960  Entropy=0.024  AvgReward=7.36\n",
      "Epoch 751/1000  Loss=9.742  PolicyLoss=-1.772  ValueLoss=23.032  Entropy=0.024  AvgReward=7.26\n",
      "Epoch 752/1000  Loss=9.109  PolicyLoss=-2.185  ValueLoss=22.593  Entropy=0.024  AvgReward=7.68\n",
      "Epoch 753/1000  Loss=9.762  PolicyLoss=-1.739  ValueLoss=23.005  Entropy=0.023  AvgReward=7.24\n",
      "Epoch 754/1000  Loss=10.149  PolicyLoss=-1.472  ValueLoss=23.246  Entropy=0.023  AvgReward=6.98\n",
      "Epoch 755/1000  Loss=9.843  PolicyLoss=-1.665  ValueLoss=23.020  Entropy=0.023  AvgReward=7.18\n",
      "Epoch 756/1000  Loss=10.082  PolicyLoss=-1.499  ValueLoss=23.165  Entropy=0.023  AvgReward=7.02\n",
      "Epoch 757/1000  Loss=9.070  PolicyLoss=-2.152  ValueLoss=22.449  Entropy=0.023  AvgReward=7.68\n",
      "Epoch 758/1000  Loss=10.229  PolicyLoss=-1.385  ValueLoss=23.233  Entropy=0.022  AvgReward=6.92\n",
      "Epoch 759/1000  Loss=10.041  PolicyLoss=-1.499  ValueLoss=23.085  Entropy=0.022  AvgReward=7.04\n",
      "Epoch 760/1000  Loss=9.853  PolicyLoss=-1.613  ValueLoss=22.935  Entropy=0.022  AvgReward=7.16\n",
      "Epoch 761/1000  Loss=9.414  PolicyLoss=-1.886  ValueLoss=22.604  Entropy=0.022  AvgReward=7.44\n",
      "Epoch 762/1000  Loss=10.063  PolicyLoss=-1.460  ValueLoss=23.050  Entropy=0.022  AvgReward=7.02\n",
      "Epoch 763/1000  Loss=9.841  PolicyLoss=-1.593  ValueLoss=22.873  Entropy=0.021  AvgReward=7.16\n",
      "Epoch 764/1000  Loss=9.523  PolicyLoss=-1.787  ValueLoss=22.623  Entropy=0.021  AvgReward=7.36\n",
      "Epoch 765/1000  Loss=9.928  PolicyLoss=-1.521  ValueLoss=22.902  Entropy=0.021  AvgReward=7.10\n",
      "Epoch 766/1000  Loss=9.418  PolicyLoss=-1.834  ValueLoss=22.508  Entropy=0.021  AvgReward=7.42\n",
      "Epoch 767/1000  Loss=9.476  PolicyLoss=-1.788  ValueLoss=22.532  Entropy=0.021  AvgReward=7.38\n",
      "Epoch 768/1000  Loss=10.047  PolicyLoss=-1.421  ValueLoss=22.940  Entropy=0.021  AvgReward=7.02\n",
      "Epoch 769/1000  Loss=10.108  PolicyLoss=-1.375  ValueLoss=22.970  Entropy=0.020  AvgReward=6.98\n",
      "Epoch 770/1000  Loss=9.558  PolicyLoss=-1.709  ValueLoss=22.538  Entropy=0.020  AvgReward=7.32\n",
      "Epoch 771/1000  Loss=10.136  PolicyLoss=-1.343  ValueLoss=22.961  Entropy=0.020  AvgReward=6.96\n",
      "Epoch 772/1000  Loss=9.549  PolicyLoss=-1.696  ValueLoss=22.495  Entropy=0.020  AvgReward=7.32\n",
      "Epoch 773/1000  Loss=9.936  PolicyLoss=-1.450  ValueLoss=22.777  Entropy=0.020  AvgReward=7.08\n",
      "Epoch 774/1000  Loss=9.377  PolicyLoss=-1.784  ValueLoss=22.326  Entropy=0.020  AvgReward=7.42\n",
      "Epoch 775/1000  Loss=9.537  PolicyLoss=-1.678  ValueLoss=22.433  Entropy=0.019  AvgReward=7.32\n",
      "Epoch 776/1000  Loss=9.928  PolicyLoss=-1.432  ValueLoss=22.723  Entropy=0.019  AvgReward=7.08\n",
      "Epoch 777/1000  Loss=9.958  PolicyLoss=-1.405  ValueLoss=22.732  Entropy=0.019  AvgReward=7.06\n",
      "Epoch 778/1000  Loss=9.724  PolicyLoss=-1.539  ValueLoss=22.529  Entropy=0.019  AvgReward=7.20\n",
      "Epoch 779/1000  Loss=9.587  PolicyLoss=-1.613  ValueLoss=22.404  Entropy=0.019  AvgReward=7.28\n",
      "Epoch 780/1000  Loss=9.650  PolicyLoss=-1.567  ValueLoss=22.438  Entropy=0.019  AvgReward=7.24\n",
      "Epoch 781/1000  Loss=9.479  PolicyLoss=-1.661  ValueLoss=22.283  Entropy=0.019  AvgReward=7.34\n",
      "Epoch 782/1000  Loss=9.643  PolicyLoss=-1.555  ValueLoss=22.400  Entropy=0.018  AvgReward=7.24\n",
      "Epoch 783/1000  Loss=10.181  PolicyLoss=-1.229  ValueLoss=22.823  Entropy=0.018  AvgReward=6.92\n",
      "Epoch 784/1000  Loss=9.806  PolicyLoss=-1.443  ValueLoss=22.501  Entropy=0.018  AvgReward=7.14\n",
      "Epoch 785/1000  Loss=9.736  PolicyLoss=-1.477  ValueLoss=22.428  Entropy=0.018  AvgReward=7.18\n",
      "Epoch 786/1000  Loss=10.246  PolicyLoss=-1.171  ValueLoss=22.836  Entropy=0.018  AvgReward=6.88\n",
      "Epoch 787/1000  Loss=9.593  PolicyLoss=-1.545  ValueLoss=22.278  Entropy=0.018  AvgReward=7.26\n",
      "Epoch 788/1000  Loss=9.383  PolicyLoss=-1.659  ValueLoss=22.087  Entropy=0.017  AvgReward=7.38\n",
      "Epoch 789/1000  Loss=9.897  PolicyLoss=-1.353  ValueLoss=22.504  Entropy=0.017  AvgReward=7.08\n",
      "Epoch 790/1000  Loss=9.895  PolicyLoss=-1.347  ValueLoss=22.488  Entropy=0.017  AvgReward=7.08\n",
      "Epoch 791/1000  Loss=9.441  PolicyLoss=-1.601  ValueLoss=22.087  Entropy=0.017  AvgReward=7.34\n",
      "Epoch 792/1000  Loss=10.031  PolicyLoss=-1.255  ValueLoss=22.575  Entropy=0.017  AvgReward=7.00\n",
      "Epoch 793/1000  Loss=9.119  PolicyLoss=-1.769  ValueLoss=21.780  Entropy=0.017  AvgReward=7.52\n",
      "Epoch 794/1000  Loss=9.150  PolicyLoss=-1.743  ValueLoss=21.789  Entropy=0.016  AvgReward=7.50\n",
      "Epoch 795/1000  Loss=9.709  PolicyLoss=-1.417  ValueLoss=22.256  Entropy=0.016  AvgReward=7.18\n",
      "Epoch 796/1000  Loss=10.025  PolicyLoss=-1.231  ValueLoss=22.516  Entropy=0.016  AvgReward=7.00\n",
      "Epoch 797/1000  Loss=9.207  PolicyLoss=-1.685  ValueLoss=21.788  Entropy=0.016  AvgReward=7.46\n",
      "Epoch 798/1000  Loss=9.773  PolicyLoss=-1.359  ValueLoss=22.268  Entropy=0.016  AvgReward=7.14\n",
      "Epoch 799/1000  Loss=9.700  PolicyLoss=-1.393  ValueLoss=22.188  Entropy=0.016  AvgReward=7.18\n",
      "Epoch 800/1000  Loss=9.374  PolicyLoss=-1.567  ValueLoss=21.887  Entropy=0.016  AvgReward=7.36\n",
      "Epoch 801/1000  Loss=9.875  PolicyLoss=-1.281  ValueLoss=22.315  Entropy=0.015  AvgReward=7.08\n",
      "Epoch 802/1000  Loss=8.863  PolicyLoss=-1.835  ValueLoss=21.399  Entropy=0.015  AvgReward=7.64\n",
      "Epoch 803/1000  Loss=9.546  PolicyLoss=-1.449  ValueLoss=21.993  Entropy=0.015  AvgReward=7.26\n",
      "Epoch 804/1000  Loss=9.325  PolicyLoss=-1.563  ValueLoss=21.779  Entropy=0.015  AvgReward=7.38\n",
      "Epoch 805/1000  Loss=9.467  PolicyLoss=-1.477  ValueLoss=21.892  Entropy=0.015  AvgReward=7.30\n",
      "Epoch 806/1000  Loss=9.501  PolicyLoss=-1.451  ValueLoss=21.907  Entropy=0.015  AvgReward=7.28\n",
      "Epoch 807/1000  Loss=10.379  PolicyLoss=-0.965  ValueLoss=22.691  Entropy=0.015  AvgReward=6.80\n",
      "Epoch 808/1000  Loss=8.980  PolicyLoss=-1.719  ValueLoss=21.401  Entropy=0.014  AvgReward=7.56\n",
      "Epoch 809/1000  Loss=9.087  PolicyLoss=-1.653  ValueLoss=21.482  Entropy=0.014  AvgReward=7.50\n",
      "Epoch 810/1000  Loss=9.490  PolicyLoss=-1.427  ValueLoss=21.837  Entropy=0.014  AvgReward=7.28\n",
      "Epoch 811/1000  Loss=9.488  PolicyLoss=-1.421  ValueLoss=21.820  Entropy=0.014  AvgReward=7.28\n",
      "Epoch 812/1000  Loss=9.896  PolicyLoss=-1.195  ValueLoss=22.184  Entropy=0.014  AvgReward=7.06\n",
      "Epoch 813/1000  Loss=9.371  PolicyLoss=-1.469  ValueLoss=21.682  Entropy=0.014  AvgReward=7.34\n",
      "Epoch 814/1000  Loss=9.293  PolicyLoss=-1.503  ValueLoss=21.594  Entropy=0.014  AvgReward=7.38\n",
      "Epoch 815/1000  Loss=9.591  PolicyLoss=-1.337  ValueLoss=21.859  Entropy=0.013  AvgReward=7.22\n",
      "Epoch 816/1000  Loss=9.363  PolicyLoss=-1.451  ValueLoss=21.630  Entropy=0.013  AvgReward=7.34\n",
      "Epoch 817/1000  Loss=9.777  PolicyLoss=-1.225  ValueLoss=22.006  Entropy=0.013  AvgReward=7.12\n",
      "Epoch 818/1000  Loss=9.357  PolicyLoss=-1.439  ValueLoss=21.596  Entropy=0.013  AvgReward=7.34\n",
      "Epoch 819/1000  Loss=9.927  PolicyLoss=-1.133  ValueLoss=22.123  Entropy=0.013  AvgReward=7.04\n",
      "Epoch 820/1000  Loss=9.429  PolicyLoss=-1.387  ValueLoss=21.635  Entropy=0.013  AvgReward=7.30\n",
      "Epoch 821/1000  Loss=9.503  PolicyLoss=-1.342  ValueLoss=21.692  Entropy=0.013  AvgReward=7.26\n",
      "Epoch 822/1000  Loss=9.386  PolicyLoss=-1.396  ValueLoss=21.566  Entropy=0.013  AvgReward=7.32\n",
      "Epoch 823/1000  Loss=8.998  PolicyLoss=-1.590  ValueLoss=21.178  Entropy=0.012  AvgReward=7.52\n",
      "Epoch 824/1000  Loss=10.040  PolicyLoss=-1.044  ValueLoss=22.170  Entropy=0.012  AvgReward=6.98\n",
      "Epoch 825/1000  Loss=10.039  PolicyLoss=-1.038  ValueLoss=22.158  Entropy=0.012  AvgReward=6.98\n",
      "Epoch 826/1000  Loss=10.156  PolicyLoss=-0.973  ValueLoss=22.260  Entropy=0.012  AvgReward=6.92\n",
      "Epoch 827/1000  Loss=9.531  PolicyLoss=-1.287  ValueLoss=21.640  Entropy=0.012  AvgReward=7.24\n",
      "Epoch 828/1000  Loss=9.138  PolicyLoss=-1.482  ValueLoss=21.242  Entropy=0.012  AvgReward=7.44\n",
      "Epoch 829/1000  Loss=9.410  PolicyLoss=-1.336  ValueLoss=21.496  Entropy=0.012  AvgReward=7.30\n",
      "Epoch 830/1000  Loss=8.857  PolicyLoss=-1.611  ValueLoss=20.938  Entropy=0.012  AvgReward=7.58\n",
      "Epoch 831/1000  Loss=10.236  PolicyLoss=-0.905  ValueLoss=22.285  Entropy=0.012  AvgReward=6.88\n",
      "Epoch 832/1000  Loss=8.930  PolicyLoss=-1.559  ValueLoss=20.980  Entropy=0.011  AvgReward=7.54\n",
      "Epoch 833/1000  Loss=9.244  PolicyLoss=-1.394  ValueLoss=21.278  Entropy=0.011  AvgReward=7.38\n",
      "Epoch 834/1000  Loss=9.760  PolicyLoss=-1.128  ValueLoss=21.778  Entropy=0.011  AvgReward=7.12\n",
      "Epoch 835/1000  Loss=9.559  PolicyLoss=-1.222  ValueLoss=21.566  Entropy=0.011  AvgReward=7.22\n",
      "Epoch 836/1000  Loss=9.278  PolicyLoss=-1.357  ValueLoss=21.271  Entropy=0.011  AvgReward=7.36\n",
      "Epoch 837/1000  Loss=9.115  PolicyLoss=-1.431  ValueLoss=21.095  Entropy=0.011  AvgReward=7.44\n",
      "Epoch 838/1000  Loss=10.120  PolicyLoss=-0.925  ValueLoss=22.093  Entropy=0.011  AvgReward=6.94\n",
      "Epoch 839/1000  Loss=9.676  PolicyLoss=-1.140  ValueLoss=21.634  Entropy=0.011  AvgReward=7.16\n",
      "Epoch 840/1000  Loss=9.230  PolicyLoss=-1.354  ValueLoss=21.170  Entropy=0.011  AvgReward=7.38\n",
      "Epoch 841/1000  Loss=9.634  PolicyLoss=-1.149  ValueLoss=21.568  Entropy=0.010  AvgReward=7.18\n",
      "Epoch 842/1000  Loss=10.285  PolicyLoss=-0.824  ValueLoss=22.219  Entropy=0.010  AvgReward=6.86\n",
      "Epoch 843/1000  Loss=10.694  PolicyLoss=-0.618  ValueLoss=22.627  Entropy=0.010  AvgReward=6.66\n",
      "Epoch 844/1000  Loss=9.591  PolicyLoss=-1.153  ValueLoss=21.490  Entropy=0.010  AvgReward=7.20\n",
      "Epoch 845/1000  Loss=9.262  PolicyLoss=-1.308  ValueLoss=21.142  Entropy=0.010  AvgReward=7.36\n",
      "Epoch 846/1000  Loss=9.178  PolicyLoss=-1.343  ValueLoss=21.044  Entropy=0.010  AvgReward=7.40\n",
      "Epoch 847/1000  Loss=10.001  PolicyLoss=-0.938  ValueLoss=21.879  Entropy=0.010  AvgReward=7.00\n",
      "Epoch 848/1000  Loss=9.795  PolicyLoss=-1.033  ValueLoss=21.656  Entropy=0.010  AvgReward=7.10\n",
      "Epoch 849/1000  Loss=9.380  PolicyLoss=-1.228  ValueLoss=21.217  Entropy=0.010  AvgReward=7.30\n",
      "Epoch 850/1000  Loss=9.919  PolicyLoss=-0.963  ValueLoss=21.765  Entropy=0.010  AvgReward=7.04\n",
      "Epoch 851/1000  Loss=9.128  PolicyLoss=-1.337  ValueLoss=20.932  Entropy=0.010  AvgReward=7.42\n",
      "Epoch 852/1000  Loss=10.086  PolicyLoss=-0.872  ValueLoss=21.919  Entropy=0.009  AvgReward=6.96\n",
      "Epoch 853/1000  Loss=8.790  PolicyLoss=-1.487  ValueLoss=20.556  Entropy=0.009  AvgReward=7.58\n",
      "Epoch 854/1000  Loss=9.962  PolicyLoss=-0.922  ValueLoss=21.770  Entropy=0.009  AvgReward=7.02\n",
      "Epoch 855/1000  Loss=10.425  PolicyLoss=-0.697  ValueLoss=22.246  Entropy=0.009  AvgReward=6.80\n",
      "Epoch 856/1000  Loss=9.330  PolicyLoss=-1.212  ValueLoss=21.087  Entropy=0.009  AvgReward=7.32\n",
      "Epoch 857/1000  Loss=9.414  PolicyLoss=-1.167  ValueLoss=21.164  Entropy=0.009  AvgReward=7.28\n",
      "Epoch 858/1000  Loss=10.175  PolicyLoss=-0.802  ValueLoss=21.957  Entropy=0.009  AvgReward=6.92\n",
      "Epoch 859/1000  Loss=9.752  PolicyLoss=-0.997  ValueLoss=21.500  Entropy=0.009  AvgReward=7.12\n",
      "Epoch 860/1000  Loss=8.943  PolicyLoss=-1.372  ValueLoss=20.634  Entropy=0.009  AvgReward=7.50\n",
      "Epoch 861/1000  Loss=9.368  PolicyLoss=-1.167  ValueLoss=21.073  Entropy=0.009  AvgReward=7.30\n",
      "Epoch 862/1000  Loss=9.667  PolicyLoss=-1.022  ValueLoss=21.380  Entropy=0.009  AvgReward=7.16\n",
      "Epoch 863/1000  Loss=9.109  PolicyLoss=-1.277  ValueLoss=20.775  Entropy=0.008  AvgReward=7.42\n",
      "Epoch 864/1000  Loss=9.538  PolicyLoss=-1.072  ValueLoss=21.222  Entropy=0.008  AvgReward=7.22\n",
      "Epoch 865/1000  Loss=8.891  PolicyLoss=-1.367  ValueLoss=20.519  Entropy=0.008  AvgReward=7.52\n",
      "Epoch 866/1000  Loss=9.580  PolicyLoss=-1.042  ValueLoss=21.246  Entropy=0.008  AvgReward=7.20\n",
      "Epoch 867/1000  Loss=9.666  PolicyLoss=-0.997  ValueLoss=21.328  Entropy=0.008  AvgReward=7.16\n",
      "Epoch 868/1000  Loss=10.360  PolicyLoss=-0.672  ValueLoss=22.066  Entropy=0.008  AvgReward=6.84\n",
      "Epoch 869/1000  Loss=9.188  PolicyLoss=-1.207  ValueLoss=20.792  Entropy=0.008  AvgReward=7.38\n",
      "Epoch 870/1000  Loss=9.449  PolicyLoss=-1.082  ValueLoss=21.063  Entropy=0.008  AvgReward=7.26\n",
      "Epoch 871/1000  Loss=9.623  PolicyLoss=-0.997  ValueLoss=21.242  Entropy=0.008  AvgReward=7.18\n",
      "Epoch 872/1000  Loss=8.879  PolicyLoss=-1.332  ValueLoss=20.424  Entropy=0.008  AvgReward=7.52\n",
      "Epoch 873/1000  Loss=9.579  PolicyLoss=-1.007  ValueLoss=21.174  Entropy=0.008  AvgReward=7.20\n",
      "Epoch 874/1000  Loss=9.491  PolicyLoss=-1.042  ValueLoss=21.068  Entropy=0.008  AvgReward=7.24\n",
      "Epoch 875/1000  Loss=9.755  PolicyLoss=-0.917  ValueLoss=21.347  Entropy=0.007  AvgReward=7.12\n",
      "Epoch 876/1000  Loss=9.182  PolicyLoss=-1.172  ValueLoss=20.709  Entropy=0.007  AvgReward=7.38\n",
      "Epoch 877/1000  Loss=9.358  PolicyLoss=-1.087  ValueLoss=20.892  Entropy=0.007  AvgReward=7.30\n",
      "Epoch 878/1000  Loss=10.555  PolicyLoss=-0.542  ValueLoss=22.196  Entropy=0.007  AvgReward=6.76\n",
      "Epoch 879/1000  Loss=9.091  PolicyLoss=-1.197  ValueLoss=20.577  Entropy=0.007  AvgReward=7.42\n",
      "Epoch 880/1000  Loss=8.956  PolicyLoss=-1.252  ValueLoss=20.418  Entropy=0.007  AvgReward=7.48\n",
      "Epoch 881/1000  Loss=9.848  PolicyLoss=-0.847  ValueLoss=21.392  Entropy=0.007  AvgReward=7.08\n",
      "Epoch 882/1000  Loss=8.551  PolicyLoss=-1.422  ValueLoss=19.948  Entropy=0.007  AvgReward=7.66\n",
      "Epoch 883/1000  Loss=8.504  PolicyLoss=-1.437  ValueLoss=19.884  Entropy=0.007  AvgReward=7.68\n",
      "Epoch 884/1000  Loss=9.176  PolicyLoss=-1.132  ValueLoss=20.617  Entropy=0.007  AvgReward=7.38\n",
      "Epoch 885/1000  Loss=10.617  PolicyLoss=-0.487  ValueLoss=22.209  Entropy=0.007  AvgReward=6.74\n",
      "Epoch 886/1000  Loss=9.039  PolicyLoss=-1.182  ValueLoss=20.443  Entropy=0.007  AvgReward=7.44\n",
      "Epoch 887/1000  Loss=9.808  PolicyLoss=-0.837  ValueLoss=21.290  Entropy=0.007  AvgReward=7.10\n",
      "Epoch 888/1000  Loss=9.627  PolicyLoss=-0.912  ValueLoss=21.079  Entropy=0.006  AvgReward=7.18\n",
      "Epoch 889/1000  Loss=9.718  PolicyLoss=-0.867  ValueLoss=21.172  Entropy=0.006  AvgReward=7.14\n",
      "Epoch 890/1000  Loss=9.309  PolicyLoss=-1.042  ValueLoss=20.704  Entropy=0.006  AvgReward=7.32\n",
      "Epoch 891/1000  Loss=9.400  PolicyLoss=-0.997  ValueLoss=20.797  Entropy=0.006  AvgReward=7.28\n",
      "Epoch 892/1000  Loss=9.263  PolicyLoss=-1.053  ValueLoss=20.632  Entropy=0.006  AvgReward=7.34\n",
      "Epoch 893/1000  Loss=9.859  PolicyLoss=-0.788  ValueLoss=21.294  Entropy=0.006  AvgReward=7.08\n",
      "Epoch 894/1000  Loss=9.263  PolicyLoss=-1.043  ValueLoss=20.612  Entropy=0.006  AvgReward=7.34\n",
      "Epoch 895/1000  Loss=9.124  PolicyLoss=-1.098  ValueLoss=20.446  Entropy=0.006  AvgReward=7.40\n",
      "Epoch 896/1000  Loss=9.908  PolicyLoss=-0.753  ValueLoss=21.324  Entropy=0.006  AvgReward=7.06\n",
      "Epoch 897/1000  Loss=9.170  PolicyLoss=-1.068  ValueLoss=20.477  Entropy=0.006  AvgReward=7.38\n",
      "Epoch 898/1000  Loss=10.096  PolicyLoss=-0.664  ValueLoss=21.520  Entropy=0.006  AvgReward=6.98\n",
      "Epoch 899/1000  Loss=9.540  PolicyLoss=-0.899  ValueLoss=20.880  Entropy=0.006  AvgReward=7.22\n",
      "Epoch 900/1000  Loss=9.448  PolicyLoss=-0.934  ValueLoss=20.765  Entropy=0.006  AvgReward=7.26\n",
      "Epoch 901/1000  Loss=9.308  PolicyLoss=-0.990  ValueLoss=20.597  Entropy=0.006  AvgReward=7.32\n",
      "Epoch 902/1000  Loss=8.981  PolicyLoss=-1.125  ValueLoss=20.214  Entropy=0.006  AvgReward=7.46\n",
      "Epoch 903/1000  Loss=9.496  PolicyLoss=-0.900  ValueLoss=20.793  Entropy=0.005  AvgReward=7.24\n",
      "Epoch 904/1000  Loss=9.074  PolicyLoss=-1.075  ValueLoss=20.300  Entropy=0.005  AvgReward=7.42\n",
      "Epoch 905/1000  Loss=9.873  PolicyLoss=-0.731  ValueLoss=21.208  Entropy=0.005  AvgReward=7.08\n",
      "Epoch 906/1000  Loss=9.356  PolicyLoss=-0.946  ValueLoss=20.605  Entropy=0.005  AvgReward=7.30\n",
      "Epoch 907/1000  Loss=9.262  PolicyLoss=-0.981  ValueLoss=20.487  Entropy=0.005  AvgReward=7.34\n",
      "Epoch 908/1000  Loss=9.262  PolicyLoss=-0.977  ValueLoss=20.478  Entropy=0.005  AvgReward=7.34\n",
      "Epoch 909/1000  Loss=9.167  PolicyLoss=-1.012  ValueLoss=20.360  Entropy=0.005  AvgReward=7.38\n",
      "Epoch 910/1000  Loss=8.835  PolicyLoss=-1.147  ValueLoss=19.966  Entropy=0.005  AvgReward=7.52\n",
      "Epoch 911/1000  Loss=10.118  PolicyLoss=-0.602  ValueLoss=21.442  Entropy=0.005  AvgReward=6.98\n",
      "Epoch 912/1000  Loss=9.930  PolicyLoss=-0.678  ValueLoss=21.216  Entropy=0.005  AvgReward=7.06\n",
      "Epoch 913/1000  Loss=9.167  PolicyLoss=-0.993  ValueLoss=20.322  Entropy=0.005  AvgReward=7.38\n",
      "Epoch 914/1000  Loss=9.215  PolicyLoss=-0.969  ValueLoss=20.368  Entropy=0.005  AvgReward=7.36\n",
      "Epoch 915/1000  Loss=10.030  PolicyLoss=-0.624  ValueLoss=21.309  Entropy=0.005  AvgReward=7.02\n",
      "Epoch 916/1000  Loss=10.272  PolicyLoss=-0.519  ValueLoss=21.583  Entropy=0.005  AvgReward=6.92\n",
      "Epoch 917/1000  Loss=10.370  PolicyLoss=-0.475  ValueLoss=21.691  Entropy=0.005  AvgReward=6.88\n",
      "Epoch 918/1000  Loss=8.349  PolicyLoss=-1.311  ValueLoss=19.320  Entropy=0.005  AvgReward=7.72\n",
      "Epoch 919/1000  Loss=9.264  PolicyLoss=-0.926  ValueLoss=20.383  Entropy=0.005  AvgReward=7.34\n",
      "Epoch 920/1000  Loss=10.377  PolicyLoss=-0.462  ValueLoss=21.679  Entropy=0.004  AvgReward=6.88\n",
      "Epoch 921/1000  Loss=9.120  PolicyLoss=-0.978  ValueLoss=20.196  Entropy=0.004  AvgReward=7.40\n",
      "Epoch 922/1000  Loss=8.780  PolicyLoss=-1.113  ValueLoss=19.788  Entropy=0.004  AvgReward=7.54\n",
      "Epoch 923/1000  Loss=9.120  PolicyLoss=-0.969  ValueLoss=20.179  Entropy=0.004  AvgReward=7.40\n",
      "Epoch 924/1000  Loss=9.656  PolicyLoss=-0.744  ValueLoss=20.802  Entropy=0.004  AvgReward=7.18\n",
      "Epoch 925/1000  Loss=10.096  PolicyLoss=-0.560  ValueLoss=21.314  Entropy=0.004  AvgReward=7.00\n",
      "Epoch 926/1000  Loss=9.121  PolicyLoss=-0.956  ValueLoss=20.153  Entropy=0.004  AvgReward=7.40\n",
      "Epoch 927/1000  Loss=10.345  PolicyLoss=-0.451  ValueLoss=21.594  Entropy=0.004  AvgReward=6.90\n",
      "Epoch 928/1000  Loss=8.974  PolicyLoss=-1.007  ValueLoss=19.963  Entropy=0.004  AvgReward=7.46\n",
      "Epoch 929/1000  Loss=10.006  PolicyLoss=-0.583  ValueLoss=21.178  Entropy=0.004  AvgReward=7.04\n",
      "Epoch 930/1000  Loss=8.875  PolicyLoss=-1.039  ValueLoss=19.829  Entropy=0.004  AvgReward=7.50\n",
      "Epoch 931/1000  Loss=9.171  PolicyLoss=-0.914  ValueLoss=20.172  Entropy=0.004  AvgReward=7.38\n",
      "Epoch 932/1000  Loss=9.073  PolicyLoss=-0.950  ValueLoss=20.046  Entropy=0.004  AvgReward=7.42\n",
      "Epoch 933/1000  Loss=9.914  PolicyLoss=-0.606  ValueLoss=21.040  Entropy=0.004  AvgReward=7.08\n",
      "Epoch 934/1000  Loss=9.123  PolicyLoss=-0.921  ValueLoss=20.089  Entropy=0.004  AvgReward=7.40\n",
      "Epoch 935/1000  Loss=9.322  PolicyLoss=-0.837  ValueLoss=20.318  Entropy=0.004  AvgReward=7.32\n",
      "Epoch 936/1000  Loss=9.521  PolicyLoss=-0.753  ValueLoss=20.549  Entropy=0.004  AvgReward=7.24\n",
      "Epoch 937/1000  Loss=8.974  PolicyLoss=-0.968  ValueLoss=19.886  Entropy=0.004  AvgReward=7.46\n",
      "Epoch 938/1000  Loss=9.474  PolicyLoss=-0.764  ValueLoss=20.476  Entropy=0.004  AvgReward=7.26\n",
      "Epoch 939/1000  Loss=9.175  PolicyLoss=-0.879  ValueLoss=20.109  Entropy=0.004  AvgReward=7.38\n",
      "Epoch 940/1000  Loss=10.378  PolicyLoss=-0.395  ValueLoss=21.546  Entropy=0.004  AvgReward=6.90\n",
      "Epoch 941/1000  Loss=10.029  PolicyLoss=-0.531  ValueLoss=21.120  Entropy=0.004  AvgReward=7.04\n",
      "Epoch 942/1000  Loss=9.478  PolicyLoss=-0.747  ValueLoss=20.450  Entropy=0.003  AvgReward=7.26\n",
      "Epoch 943/1000  Loss=8.824  PolicyLoss=-1.003  ValueLoss=19.655  Entropy=0.003  AvgReward=7.52\n",
      "Epoch 944/1000  Loss=8.371  PolicyLoss=-1.179  ValueLoss=19.099  Entropy=0.003  AvgReward=7.70\n",
      "Epoch 945/1000  Loss=9.229  PolicyLoss=-0.834  ValueLoss=20.126  Entropy=0.003  AvgReward=7.36\n",
      "Epoch 946/1000  Loss=9.685  PolicyLoss=-0.650  ValueLoss=20.670  Entropy=0.003  AvgReward=7.18\n",
      "Epoch 947/1000  Loss=9.382  PolicyLoss=-0.766  ValueLoss=20.296  Entropy=0.003  AvgReward=7.30\n",
      "Epoch 948/1000  Loss=9.586  PolicyLoss=-0.681  ValueLoss=20.536  Entropy=0.003  AvgReward=7.22\n",
      "Epoch 949/1000  Loss=10.503  PolicyLoss=-0.317  ValueLoss=21.641  Entropy=0.003  AvgReward=6.86\n",
      "Epoch 950/1000  Loss=9.334  PolicyLoss=-0.773  ValueLoss=20.215  Entropy=0.003  AvgReward=7.32\n",
      "Epoch 951/1000  Loss=10.049  PolicyLoss=-0.489  ValueLoss=21.078  Entropy=0.003  AvgReward=7.04\n",
      "Epoch 952/1000  Loss=9.489  PolicyLoss=-0.705  ValueLoss=20.390  Entropy=0.003  AvgReward=7.26\n",
      "Epoch 953/1000  Loss=9.797  PolicyLoss=-0.582  ValueLoss=20.759  Entropy=0.003  AvgReward=7.14\n",
      "Epoch 954/1000  Loss=8.979  PolicyLoss=-0.898  ValueLoss=19.754  Entropy=0.003  AvgReward=7.46\n",
      "Epoch 955/1000  Loss=9.493  PolicyLoss=-0.694  ValueLoss=20.374  Entropy=0.003  AvgReward=7.26\n",
      "Epoch 956/1000  Loss=8.003  PolicyLoss=-1.270  ValueLoss=18.547  Entropy=0.003  AvgReward=7.84\n",
      "Epoch 957/1000  Loss=8.929  PolicyLoss=-0.906  ValueLoss=19.670  Entropy=0.003  AvgReward=7.48\n",
      "Epoch 958/1000  Loss=10.219  PolicyLoss=-0.402  ValueLoss=21.241  Entropy=0.003  AvgReward=6.98\n",
      "Epoch 959/1000  Loss=9.601  PolicyLoss=-0.638  ValueLoss=20.478  Entropy=0.003  AvgReward=7.22\n",
      "Epoch 960/1000  Loss=10.017  PolicyLoss=-0.474  ValueLoss=20.981  Entropy=0.003  AvgReward=7.06\n",
      "Epoch 961/1000  Loss=9.760  PolicyLoss=-0.570  ValueLoss=20.659  Entropy=0.003  AvgReward=7.16\n",
      "Epoch 962/1000  Loss=8.775  PolicyLoss=-0.946  ValueLoss=19.443  Entropy=0.003  AvgReward=7.54\n",
      "Epoch 963/1000  Loss=10.283  PolicyLoss=-0.362  ValueLoss=21.289  Entropy=0.003  AvgReward=6.96\n",
      "Epoch 964/1000  Loss=9.817  PolicyLoss=-0.538  ValueLoss=20.710  Entropy=0.003  AvgReward=7.14\n",
      "Epoch 965/1000  Loss=9.610  PolicyLoss=-0.614  ValueLoss=20.449  Entropy=0.003  AvgReward=7.22\n",
      "Epoch 966/1000  Loss=9.716  PolicyLoss=-0.570  ValueLoss=20.573  Entropy=0.003  AvgReward=7.18\n",
      "Epoch 967/1000  Loss=10.135  PolicyLoss=-0.407  ValueLoss=21.085  Entropy=0.003  AvgReward=7.02\n",
      "Epoch 968/1000  Loss=9.143  PolicyLoss=-0.783  ValueLoss=19.854  Entropy=0.003  AvgReward=7.40\n",
      "Epoch 969/1000  Loss=8.463  PolicyLoss=-1.040  ValueLoss=19.005  Entropy=0.003  AvgReward=7.66\n",
      "Epoch 970/1000  Loss=9.827  PolicyLoss=-0.516  ValueLoss=20.686  Entropy=0.003  AvgReward=7.14\n",
      "Epoch 971/1000  Loss=9.251  PolicyLoss=-0.732  ValueLoss=19.966  Entropy=0.002  AvgReward=7.36\n",
      "Epoch 972/1000  Loss=8.515  PolicyLoss=-1.008  ValueLoss=19.047  Entropy=0.002  AvgReward=7.64\n",
      "Epoch 973/1000  Loss=8.620  PolicyLoss=-0.964  ValueLoss=19.170  Entropy=0.002  AvgReward=7.60\n",
      "Epoch 974/1000  Loss=8.937  PolicyLoss=-0.840  ValueLoss=19.556  Entropy=0.002  AvgReward=7.48\n",
      "Epoch 975/1000  Loss=9.202  PolicyLoss=-0.736  ValueLoss=19.877  Entropy=0.002  AvgReward=7.38\n",
      "Epoch 976/1000  Loss=8.992  PolicyLoss=-0.812  ValueLoss=19.608  Entropy=0.002  AvgReward=7.46\n",
      "Epoch 977/1000  Loss=9.205  PolicyLoss=-0.728  ValueLoss=19.865  Entropy=0.002  AvgReward=7.38\n",
      "Epoch 978/1000  Loss=9.896  PolicyLoss=-0.464  ValueLoss=20.720  Entropy=0.002  AvgReward=7.12\n",
      "Epoch 979/1000  Loss=8.249  PolicyLoss=-1.079  ValueLoss=18.658  Entropy=0.002  AvgReward=7.74\n",
      "Epoch 980/1000  Loss=9.634  PolicyLoss=-0.555  ValueLoss=20.380  Entropy=0.002  AvgReward=7.22\n",
      "Epoch 981/1000  Loss=9.476  PolicyLoss=-0.611  ValueLoss=20.175  Entropy=0.002  AvgReward=7.28\n",
      "Epoch 982/1000  Loss=10.173  PolicyLoss=-0.347  ValueLoss=21.040  Entropy=0.002  AvgReward=7.02\n",
      "Epoch 983/1000  Loss=8.837  PolicyLoss=-0.843  ValueLoss=19.360  Entropy=0.002  AvgReward=7.52\n",
      "Epoch 984/1000  Loss=9.803  PolicyLoss=-0.479  ValueLoss=20.564  Entropy=0.002  AvgReward=7.16\n",
      "Epoch 985/1000  Loss=9.536  PolicyLoss=-0.575  ValueLoss=20.223  Entropy=0.002  AvgReward=7.26\n",
      "Epoch 986/1000  Loss=9.323  PolicyLoss=-0.651  ValueLoss=19.949  Entropy=0.002  AvgReward=7.34\n",
      "Epoch 987/1000  Loss=9.647  PolicyLoss=-0.528  ValueLoss=20.350  Entropy=0.002  AvgReward=7.22\n",
      "Epoch 988/1000  Loss=9.919  PolicyLoss=-0.424  ValueLoss=20.685  Entropy=0.002  AvgReward=7.12\n",
      "Epoch 989/1000  Loss=8.517  PolicyLoss=-0.940  ValueLoss=18.914  Entropy=0.002  AvgReward=7.64\n",
      "Epoch 990/1000  Loss=7.922  PolicyLoss=-1.156  ValueLoss=18.157  Entropy=0.002  AvgReward=7.86\n",
      "Epoch 991/1000  Loss=10.196  PolicyLoss=-0.312  ValueLoss=21.017  Entropy=0.002  AvgReward=7.02\n",
      "Epoch 992/1000  Loss=8.951  PolicyLoss=-0.768  ValueLoss=19.440  Entropy=0.002  AvgReward=7.48\n",
      "Epoch 993/1000  Loss=9.441  PolicyLoss=-0.584  ValueLoss=20.051  Entropy=0.002  AvgReward=7.30\n",
      "Epoch 994/1000  Loss=9.388  PolicyLoss=-0.600  ValueLoss=19.978  Entropy=0.002  AvgReward=7.32\n",
      "Epoch 995/1000  Loss=9.172  PolicyLoss=-0.676  ValueLoss=19.697  Entropy=0.002  AvgReward=7.40\n",
      "Epoch 996/1000  Loss=9.555  PolicyLoss=-0.532  ValueLoss=20.176  Entropy=0.002  AvgReward=7.26\n",
      "Epoch 997/1000  Loss=9.557  PolicyLoss=-0.528  ValueLoss=20.172  Entropy=0.002  AvgReward=7.26\n",
      "Epoch 998/1000  Loss=10.106  PolicyLoss=-0.325  ValueLoss=20.862  Entropy=0.002  AvgReward=7.06\n",
      "Epoch 999/1000  Loss=9.177  PolicyLoss=-0.661  ValueLoss=19.677  Entropy=0.002  AvgReward=7.40\n",
      "Epoch 1000/1000  Loss=9.179  PolicyLoss=-0.657  ValueLoss=19.672  Entropy=0.002  AvgReward=7.40\n",
      "\n",
      "Evaluación final: Recompensa media en 1000 episodios = 7.52\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
